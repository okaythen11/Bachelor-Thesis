% Encoding: UTF-8

%%%
%
%Beim Erstellen der Bibtex-Datei wird empfohlen darauf zu achten, dass die DOI aufgeführt wird.
%
%%%
@online{stable-diffusion,
	author    = {},
	title     = {},
	year      = {2025},
	url       = {https://stability.ai/},
	note      = {Accessed: 2025-06-16}
}
@online{midjourney,
	author    = {},
	title     = {},
	year      = {2025},
	url       = {https://midjourney.online/},
	note      = {Accessed: 2025-06-16}
}
@online{GPT-4o,
	author    = {},
	title     = {},
	year      = {2025},
	url       = {https://chatgpt.com/},
	note      = {Accessed: 2025-06-16}
}
@misc{AI-users,
	key = AI-users,
	note = 0, 
	url=https://www.statista.com/forecasts/1449844/ai-tool-users-worldwide
}


@book{WSPA,
	author={Sanjiva Weerawarana and Francisco Curbera and Frank Leymann and Tony Storey and Donald F. Ferguson},
	title={Web Services Platform Architecture : SOAP, WSDL, WS-Policy, WS-Addressing, WS-BPEL, WS-Reliable Messaging, and More},
	year={2005},
	price={$31.49},
	publisher={Prentice Hall PTR},
	isbn={0131488740},
	doi = {10.1.1/jpb001}
}


% !! DO NOT USE `label =   {ASF}` in other Misc entries !!

@Misc{ApacheODE,
	author = {{The Apache Software Foundation}},
	title =  {Apache ODE\texttrademark{} -- The Orchestration Director Engine},
	year =   {2016},
	label =  {ASF},
	url =    {http://ode.apache.org}
}

@Article{RVvdA2016,
	author =    {H.A. Reijers and I. Vanderfeesten and W.M.P. van der Aalst},
	title =     {The effectiveness of workflow management systems: A longitudinal study},
	journal =   {International Journal of Information Management},
	year =      {2016},
	volume =    {36},
	number =    {1},
	pages =     {126--141},
	month =     feb,
	doi =       {10.1016/j.ijinfomgt.2015.08.003},
	publisher = {Elsevier {BV}}
}
@misc{kingma2022autoencodingvariationalbayes,
	title={Auto-Encoding Variational Bayes}, 
	author={Diederik P Kingma and Max Welling},
	year={2022},
	eprint={1312.6114},
	archivePrefix={arXiv},
	primaryClass={stat.ML},
	url={https://arxiv.org/abs/1312.6114}, 
}
@misc{goodfellow2014generativeadversarialnetworks,
	title={Generative Adversarial Networks}, 
	author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
	year={2014},
	eprint={1406.2661},
	archivePrefix={arXiv},
	primaryClass={stat.ML},
	url={https://arxiv.org/abs/1406.2661}, 
}
@misc{vaswani2023attentionneed,
	title={Attention Is All You Need}, 
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year={2023},
	eprint={1706.03762},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1706.03762}, 
}
@misc{GPT-1,
title={Impriving Language Understanding by Generative Pre-Training},
authors={Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever},
year={2018},
url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}}
@misc{devlin2019bertpretrainingdeepbidirectional,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
	author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	year={2019},
	eprint={1810.04805},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1810.04805}, 
}
@misc{parmar2018imagetransformer,
	title={Image Transformer}, 
	author={Niki Parmar and Ashish Vaswani and Jakob Uszkoreit and Łukasz Kaiser and Noam Shazeer and Alexander Ku and Dustin Tran},
	year={2018},
	eprint={1802.05751},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/1802.05751}, 
}
@misc{brown2020languagemodelsfewshotlearners,
	title={Language Models are Few-Shot Learners}, 
	author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
	year={2020},
	eprint={2005.14165},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2005.14165}, 
}
@misc{zhang2025understandingrelationshippromptsresponse,
	title={Understanding the Relationship between Prompts and Response Uncertainty in Large Language Models}, 
	author={Ze Yu Zhang and Arun Verma and Finale Doshi-Velez and Bryan Kian Hsiang Low},
	year={2025},
	eprint={2407.14845},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2407.14845}, 
}
@misc{li2024effectsdifferentpromptsquality,
	title={Effects of Different Prompts on the Quality of GPT-4 Responses to Dementia Care Questions}, 
	author={Zhuochun Li and Bo Xie and Robin Hilsabeck and Alyssa Aguirre and Ning Zou and Zhimeng Luo and Daqing He},
	year={2024},
	eprint={2404.08674},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2404.08674}, 
}
@inbook{inbook,
	author = {Lindeberg, Tony},
	year = {2012},
	month = {05},
	pages = {},
	title = {Scale Invariant Feature Transform},
	volume = {7},
	journal = {Scholarpedia},
	doi = {10.4249/scholarpedia.10491}
}
@misc{patel2019img2vec,
	author       = {Akshay Patel},
	title        = {img2vec-pytorch},
	year         = {2019},
	howpublished = {\url{https://github.com/christiansafka/img2vec}},
	note         = {Version X.X.X}
}
@misc{mishra2025promptaidpromptexplorationperturbation,
	title={PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using Visual Analytics for Large Language Models}, 
	author={Aditi Mishra and Utkarsh Soni and Anjana Arunkumar and Jinbin Huang and Bum Chul Kwon and Chris Bryan},
	year={2025},
	eprint={2304.01964},
	archivePrefix={arXiv},
	primaryClass={cs.HC},
	url={https://arxiv.org/abs/2304.01964}, 
}
@misc{guo2024prompthisvisualizingprocessinfluence,
	title={PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation}, 
	author={Yuhan Guo and Hanning Shao and Can Liu and Kai Xu and Xiaoru Yuan},
	year={2024},
	eprint={2403.09615},
	archivePrefix={arXiv},
	primaryClass={cs.HC},
	url={https://arxiv.org/abs/2403.09615}, 
}
@misc{brade2023promptifytexttoimagegenerationinteractive,
	title={Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models}, 
	author={Stephen Brade and Bryan Wang and Mauricio Sousa and Sageev Oore and Tovi Grossman},
	year={2023},
	eprint={2304.09337},
	archivePrefix={arXiv},
	primaryClass={cs.HC},
	url={https://arxiv.org/abs/2304.09337}, 
}
@article{van2008visualizing,
	title={Visualizing data using t-SNE},
	author={van der Maaten, Laurens and Hinton, Geoffrey},
	journal={Journal of Machine Learning Research},
	volume={9},
	number={Nov},
	pages={2579--2605},
	year={2008}
}
@misc{lin2025sketchflexfacilitatingspatialsemanticcoherence,
	title={SketchFlex: Facilitating Spatial-Semantic Coherence in Text-to-Image Generation with Region-Based Sketches}, 
	author={Haichuan Lin and Yilin Ye and Jiazhi Xia and Wei Zeng},
	year={2025},
	eprint={2502.07556},
	archivePrefix={arXiv},
	primaryClass={cs.HC},
	url={https://arxiv.org/abs/2502.07556}, 
}
@inproceedings{10.1145/3729176.3729203,
	author = {Drosos, Ian and Williams, Jack and Sarkar, Advait and Wilson, Nicholas and Rintel, Sean and Panda, Payod},
	title = {Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks},
	year = {2025},
	isbn = {9798400713842},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3729176.3729203},
	doi = {10.1145/3729176.3729203},
	abstract = {Prompting generative AI effectively is challenging for users, particularly in expressing context for comprehension tasks like explaining spreadsheet formulas, Python code, and text passages. Through a formative survey (n = 38), we uncovered a trade-off between standardized but predictable prompting support, and context-adaptive but unpredictable support. We explore this trade-off by implementing two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC), which generates UI elements for prompt refinement based on the user’s specific prompt, and Static Prompt Refinement Control (Static PRC), which offers generic controls. Our controlled user study (n = 16) showed that the Dynamic PRC approach afforded more control, lowered barriers to providing context, and encouraged task exploration and reflection, but reasoning about the effects of generated controls on the final output remains challenging. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows.},
	booktitle = {Proceedings of the 4th Annual Symposium on Human-Computer Interaction for Work},
	articleno = {24},
	numpages = {23},
	keywords = {Dynamic UX Generation, Prompt Middleware},
	location = {
	},
	series = {CHIWORK '25}
}

@online{You2024Aesthetic,
	author    = {Junyong You and Yuan Lin and Bin Hu},
	title     = {Enhancing Aesthetic Image Generation with Reinforcement Learning Guided Prompt Optimization in Stable Diffusion},
	year      = {2024},
	doi       = {10.2139/ssrn.5178787},
	url       = {https://ssrn.com/abstract=5178787},
	note      = {SSRN Preprint}
}
@inproceedings{promptCharm,
	author = {Wang, Zhijie and Huang, Yuheng and Song, Da and Ma, Lei and Zhang, Tianyi},
	title = {PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement},
	year = {2024},
	isbn = {9798400703300},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613904.3642803},
	doi = {10.1145/3613904.3642803},
	abstract = {The recent advancements in Generative AI have significantly advanced the field of text-to-image generation. The state-of-the-art text-to-image model, Stable Diffusion, is now capable of synthesizing high-quality images with a strong sense of aesthetics. Crafting text prompts that align with the model’s interpretation and the user’s intent thus becomes crucial. However, prompting remains challenging for novice users due to the complexity of the stable diffusion model and the non-trivial efforts required for iteratively editing and refining the text prompts. To address these challenges, we propose PromptCharm, a mixed-initiative system that facilitates text-to-image creation through multi-modal prompt engineering and refinement. To assist novice users in prompting, PromptCharm first automatically refines and optimizes the user’s initial prompt. Furthermore, PromptCharm supports the user in exploring and selecting different image styles within a large database. To assist users in effectively refining their prompts and images, PromptCharm renders model explanations by visualizing the model’s attention values. If the user notices any unsatisfactory areas in the generated images, they can further refine the images through model attention adjustment or image inpainting within the rich feedback loop of PromptCharm. To evaluate the effectiveness and usability of PromptCharm, we conducted a controlled user study with 12 participants and an exploratory user study with another 12 participants. These two studies show that participants using PromptCharm were able to create images with higher quality and better aligned with the user’s expectations compared with using two variants of PromptCharm that lacked interaction or visualization support.},
	booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
	articleno = {185},
	numpages = {21},
	keywords = {Generative AI, Large Language Models, Prompt Engineering},
	location = {Honolulu, HI, USA},
	series = {CHI '24}
}
@ARTICLE{PromptMagician,
	author={Feng, Yingchaojie and Wang, Xingbo and Wong, Kam Kwai and Wang, Sijia and Lu, Yuhong and Zhu, Minfeng and Wang, Baicheng and Chen, Wei},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation}, 
	year={2024},
	volume={30},
	number={1},
	pages={295-305},
	keywords={Visualization;Semantics;Interviews;Task analysis;Electronic mail;Computational modeling;Natural language processing;Prompt engineering;text-to-image generation;image visualization},
	doi={10.1109/TVCG.2023.3327168}}

@misc{hao2023optimizingpromptstexttoimagegeneration,
	title={Optimizing Prompts for Text-to-Image Generation}, 
	author={Yaru Hao and Zewen Chi and Li Dong and Furu Wei},
	year={2023},
	eprint={2212.09611},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2212.09611}, 
}
@misc{diffusiondb,
	title={DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models}, 
	author={Zijie J. Wang and Evan Montoya and David Munechika and Haoyang Yang and Benjamin Hoover and Duen Horng Chau},
	year={2023},
	eprint={2210.14896},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2210.14896}, 
}
@article{few-shot,
	title={Generalizing from a Few Examples: A Survey on Few-Shot Learning},
	author={Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
	journal={ACM Computing Surveys},
	volume={53},
	number={3},
	pages={1--34},
	year={2020}
}
@article{in-context learning,
	title={Language Models are Few-Shot Learners},
	author={Brown, Tom B. and Mann, Benjamin and Ryder, Nick and others},
	journal={Neural Information Processing Systems},
	volume={33},
	pages={1877--1901},
	year={2020}
}
@article{chain-of-thought,
	title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
	author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={24824--24837},
	year={2022}
}
@misc{tam2024letspeakfreelystudy,
	title={Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models}, 
	author={Zhi Rui Tam and Cheng-Kuang Wu and Yi-Lin Tsai and Chieh-Yen Lin and Hung-yi Lee and Yun-Nung Chen},
	year={2024},
	eprint={2408.02442},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2408.02442}, 
}
@inproceedings{LLMoutputconstraints,
	title={“We Need Structured Output”: Towards User-centered Constraints on Large Language Model Output},
	url={http://dx.doi.org/10.1145/3613905.3650756},
	DOI={10.1145/3613905.3650756},
	booktitle={Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
	publisher={ACM},
	author={Liu, Michael Xieyang and Liu, Frederick and Fiannaca, Alexander J. and Koo, Terry and Dixon, Lucas and Terry, Michael and Cai, Carrie J.},
	year={2024},
	month=may, pages={1–9},
	collection={CHI ’24} }

@misc{ssim,
url={https://ece.uwaterloo.ca/~z70wang/research/ssim/}}
@Comment{jabref-meta: databaseType:biblatex;}
