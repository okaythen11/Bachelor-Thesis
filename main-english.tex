% !TeX document-id = {4719ec8f-70f7-4005-9222-7d90db5fa2e2}
% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-


% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iffalse
\let\ifenglisch\iftrue
\input{pre-documentclass}
\documentclass[
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-sided printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}


\usepackage[
  title={},
  author={Oliver Seiz},
  type=bachelor,
  institute=visus, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Informatik},
  examiner={Dr. Steffen Koch},
  supervisor={Jena Satkunaranjan},
  startdate={April 10, 2025},
  enddate={Oktober 10, 2025}
]{scientific-thesis-cover}

\input{acronyms}

\makeindex

\begin{document}

%tex4ht-Konvertierung verschönern
\iftex4ht
  % tell tex4ht to create pictures also for formulas starting with '$'
  % WARNING: a tex4ht run now takes forever!
  \Configure{$}{\PicMath}{\EndPicMath}{}
  %$ % <- syntax highlighting fix for emacs
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
\pagenumbering{arabic}
\Titelblatt

%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftriplepagestyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftriplepagestyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}



%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis
\section*{Abstract}

<Short summary of the thesis>

\cleardoublepage

%Solely for German courses of study
\section*{Kurzfassung}

<Kurzfassung der Arbeit>

\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

% Bei einem ungünstigen Seitenumbruch im Inhaltsverzeichnis, kann dieser mit
% \addtocontents{toc}{\protect\newpage}
% an der passenden Stelle im Fließtext erzwungen werden.

\listoffigures
\listoftables

%Wird nur bei Verwendung von der lstlisting-Umgebung mit dem "caption"-Parameter benoetigt
%\lstlistoflistings
%ansonsten:
\ifdeutsch
  \listof{Listing}{Verzeichnis der Listings}
\else
  \listof{Listing}{List of Listings}
\fi

%mittels \newfloat wurde die Algorithmus-Gleitumgebung definiert.
%Mit folgendem Befehl werden alle floats dieses Typs ausgegeben
\ifdeutsch
  \listof{Algorithmus}{Verzeichnis der Algorithmen}
\else
  \listof{Algorithmus}{List of Algorithms}
\fi
%\listofalgorithms %Ist nur für Algorithmen, die mittels \begin{algorithm} umschlossen werden, nötig

% Abkürzungsverzeichnis
\printnoidxglossaries

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Introduction}

This thesis starts with \cref{chap:k2}.

We can also typeset \verb|<text>verbatim text</text>|.
Backticks are also rendered correctly: \verb|`words in backticks`|.
With the growing capabilities of AI in recent years its usage has become more widely adopted with some sources claiming 280 million global users in 2024 \cite{AI-users} with projections of up to 1 billion users in 2030.

Besides the very popular Llms there are the generative models which have adapted a wide variety of use cases such as music, video and image generation. 

For the purpose of this thesis we will regard image generation models. These have made great advances in the past years and the best models such as mid-journey\cite{midjourney}, stable diffusion \cite{stable-diffusion}, or even GPT-4o \cite{GPT-4o} are capable of creating impressive images that are capable of creating photo realistic images and paintings in many other art styles and art forms they are limited by very little and allow people to explore their creativity. 

The most common approach in working with am image generation models is providing a textual prompt from which the model then generates an image, other prompt medias include pictures or sounds. With this approach the problem arises that the user doesn't know how the model will process the prompt exactly. For the average user the model is a black box and getting the desired image by tweaking the prompt can be a challenging and sometimes feel arbitrary, the goal here be to understand how changes in the prompt impact the generated picture. 
\chapter{Background}
\label{chap:k2}
This section serves as an introduction to topics, concepts and techniques that are relevant to understanding this thesis, introducing the topics of generative models, diffusion models Large-Language-Models (LLMs), prompt engineering, vectorization of images, vector dimensionality reduction, pixel based comparison and visualization techniques. 
\section{Generative models}
%strutcure
%Definition: Learning data distributions to generate novel samples
%Key distinction from discriminative models

%Broad applications: image synthesis, text generation, audio, video

Generative models are a relatively recent phenomenon in the field of AI, they aim to create new unseen data that fits the data they have been trained upon. The most prominent of them are image generation models with the first models capable of generating complex images emerging in 2013 they are called VAE (variational autoencoders) named after the architecture they are based on (Auto-Encoding Variational Bayes) and work by making stochastic sampling differentiable \cite{kingma2022autoencodingvariationalbayes}, however early adaptations resulted in blury and low resolution images.
Then in 2014 goodfellow et. all {goodfellow2014generativeadversarialnetworks} proposed a new process for estimating generative models using an adverserial approach, this is realized by using a two part architecture in training, a generator and a discriminator both neural networks. The generator outputs a sample from noise and the discriminator tries to classify whether that sample is a from a real data distribution or generated based on the result of the discriminator the generators weights are adjusted to result in a less distinguishable output. This approach results in more realistic higher quality images, since the generator is trained to "fool" the discriminator into thinking its output is real. 

In 2017 Vaswani et al pusblished their breakthrough paper \verb|“Attention Is All You Need”| \cite{vaswani2023attentionneed} introducing transformer architecture for AI %self-attention, positional encoding, encoder–decoder structure 
originally intended for sequence-to-sequence tasks their adaptation too generative AI was very successful, with early models using transformer architecture like GPT-1 \cite{GPT-1} and BERT \cite{devlin2019bertpretrainingdeepbidirectional} being quite capable for their respective tasks. However early transformer models for image generation such as Image Transformer \cite{parmar2018imagetransformer} struggeled with producing high quality images, while later attempts and hybrid architecture led to better performance generating images. Then in 2020 Ho et. al published their landmark paper "Denoising Diffusion Probabilistic Models" where they adpated diffusion models for image generation running on transformers.
Subsequent improvements of the approach make diffsion models the state-of-the-art for image generation, which is why they were used for image generation in this work. %add reference to stable diffusion or models being used for live generation

\section{LLMs}
Large-Language-Models (LLMs) have emerged in the field of natural language processing (nlp) but have since transcended their original purpose, most of them function by predicting the next token utilizing a transformer architecture and large scale pre-training on massive amounts of data they are able to achieve impressive results text generation and summarization to translation, reasoning, and dialogue. Their big breakthrough was in 2020 with Brown et al. publishing GPT-3 \cite{brown2020languagemodelsfewshotlearners} first showing  that scaling the number of parameters and training data substantially improves performance. However despite their impressive capabilities LLMs are faced with several drawbacks, they are computationally expensive requiring large resource hungry data centers to train and service request at scale, additionally they may show bias or inaccuracy in their responses depending on the underlying training data, which makes sourcing quality data at the required scale a major challenge LLM developers face. Another negative is their non applicability in safety critical environments due to hallucination, where the model generates plausible but incorrect outputs.

\section{prompt engineering}
In order to use the LLMs introduced in the last section we need to "prompt" them that means entering a text or an image with instructions for the LLM. However what and how we type the prompt is important for the performance of the model as zhang et al. \cite{zhang2025understandingrelationshippromptsresponse} and li et al. \cite{li2024effectsdifferentpromptsquality} have shown for their respective selected tasks. This importance of the prompt has created a whole new field of research called "prompt engineering" where people are trying to find techniques and tricks to improve the LLM output or even get past certain restrictions. 
To achieve accurate results some prompt engineering principles were also employed in this thesis.

\section{vectorization of images}
Comparing images is an old and non-trivial problem that is also afflicted by subjectivity, as humans might rate different images as more similar or dissimilar based on individual factors. Human evaluation also takes longer and is more costly which is why a mathematical method is crucial. 
One way is to transform an image into something with many pre-existing comparison methods such as a vector where mathematical similarity measurements are well established. There are several ways to turn an image into a vector such as the "classical" approach where feature extraction is hand-engineered such as SIFT \cite{inbook}, however due to greater generalizability, automatic feature extraction and usually better performance methods using specifically trained machine learning models are generally preferred. Which was also the case in this thesis, where the Img2Vec library \cite{patel2019img2vec} was utilized.

\section{vector dimensonality reduction}
In dimensionality reduction we try to reduce the dimension of a given data points while trying to preserve certain properties and relationships between the data points, in our case we are interested in the similarity of the given data points. I utilized Umap which builds a nearest neighbour graph utilizing fuzzy sets in the higher dimensional space and then tries to preserve this realtionship in lower dimensional spaces usally 2D or 3D. 
As data pojnts we utilize the vectors we get from the previous section where we try to preserver their similarity while projecting them onto 2d space thus we are able to plot them on a simple 2d graph which is visually one of the most intuitive for humans to grasp. 

\section{pixel based comparison}




LaTeX hints are provided in \cref{chap:latexhints}.

%\blinddocument

\chapter{Related Work}
This chapter gives an overview of pre-existing concepts and work related to this thesis 


\section{Prompt Engineering and Visualization}
Mishra et al. (2023) \cite{mishra2025promptaidpromptexplorationperturbation} introduced PromptAid, an interactive system for prompt exploration and iteration when using large language models. Their approach aims to support inexperienced users in creating more effective prompts. Their framework contains three semi-automated strategies: keyword perturbations (replacing or varying keywords), paraphrasing perturbations (rewriting or changing phrasing), and "selecting an optimal set of in-context few-shot examples" which they aim to visualize for the user. This allows users to easily generate prompt iterations analyze and compare the outcomes and iteratively adjust the prompt. \\

Similarly, Brade et al. (2023) \cite{brade2023promptifytexttoimagegenerationinteractive} introduced PromptiFy a framework for prompt suggestion and result visulaziation, they achieve this by utilizing three interrative main methods in their framework 1."automatic prompt extension and suggestion" here the user can first select a general theme for which the framework will query an LLM for suggestions and then an art style, then 2."image layout and clustering by similarity" where the suggested prompts will be send to the underlying model for generation and then displayed in a graph based on their similiarity (the closer the more similar), lastly
 3."automatic prompt refinement suggestions." where an LLM suggest refinements to the prompt to achieve different outcomes. 
 This process again supports the user in finding suitable prompts.\\



 Guo et al. (2024) \cite{guo2024prompthisvisualizingprocessinfluence} presented PrompTHis, a visual analytics tool that highlights the process and influence of prompt editing during text-to-image creation. They achieved this by remebering the history of prompt and image pairs and plotting them in a specially designed graph they call an Image Variant Graph. This supports the user in aquiring an understanding of the impact of prompt changes to the image, enabeling them to closer control and influence the generation of the model. 



add figures for promptaid and promptthis?
Describe relevant scientific literature related to your work.
\section{Visualization in Generative Models}


\section{Concept Drift in Generative models}
Traditionally, concept drift in machine learning refers to a change in the mapping between input data and output labels over time. A model is trained on a particular data distribution, but as the relationship between data features and model classification shifts, its predictions become less reliable. For example, a spam email detector trained on data from five years ago may struggle today because spammers use new patterns and tactics that the model never learned, causing it to missclassify more modern spam.\\



where a model has been trained under a certain data set (the training data), but the data it is now applied on has shifted from the training data set, making the model less effective 


\chapter{Conclusion and Outlook}
\label{chap:zusfas}

\section*{Outlook}

\printbibliography

All links were last followed on March 17, 2018.

\appendix
\input{latexhints-english}

\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Versicherung
\end{document}
