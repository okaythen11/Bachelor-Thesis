% !TeX document-id = {4719ec8f-70f7-4005-9222-7d90db5fa2e2}
% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-


% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iffalse
\let\ifenglisch\iftrue
\input{pre-documentclass}
\documentclass[
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-sided printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}


\usepackage[
  title={},
  author={Oliver Seiz},
  type=bachelor,
  institute=visus, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Informatik},
  examiner={Dr. Steffen Koch},
  supervisor={Jena Satkunaranjan},
  startdate={April 08, 2025},
  enddate={Oktober 13, 2025}
]{scientific-thesis-cover}

\input{acronyms}

\makeindex

\begin{document}

%tex4ht-Konvertierung verschönern
\iftex4ht
  % tell tex4ht to create pictures also for formulas starting with '$'
  % WARNING: a tex4ht run now takes forever!
  \Configure{$}{\PicMath}{\EndPicMath}{}
  %$ % <- syntax highlighting fix for emacs
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
\pagenumbering{arabic}
\Titelblatt

%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftriplepagestyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftriplepagestyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}



%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis
\section*{Abstract}

<Short summary of the thesis>

\cleardoublepage

%Solely for German courses of study
\section*{Kurzfassung}

<Kurzfassung der Arbeit>

\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

% Bei einem ungünstigen Seitenumbruch im Inhaltsverzeichnis, kann dieser mit
% \addtocontents{toc}{\protect\newpage}
% an der passenden Stelle im Fließtext erzwungen werden.

\listoffigures
\listoftables

%Wird nur bei Verwendung von der lstlisting-Umgebung mit dem "caption"-Parameter benoetigt
%\lstlistoflistings
%ansonsten:
\ifdeutsch
  \listof{Listing}{Verzeichnis der Listings}
\else
  \listof{Listing}{List of Listings}
\fi

%mittels \newfloat wurde die Algorithmus-Gleitumgebung definiert.
%Mit folgendem Befehl werden alle floats dieses Typs ausgegeben
\ifdeutsch
  \listof{Algorithmus}{Verzeichnis der Algorithmen}
\else
  \listof{Algorithmus}{List of Algorithms}
\fi
%\listofalgorithms %Ist nur für Algorithmen, die mittels \begin{algorithm} umschlossen werden, nötig

% Abkürzungsverzeichnis
\printnoidxglossaries

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Introduction}
With the growing capabilities of artificial intelligence (AI) models in recent years, their usage has become more widely adopted, with some sources claiming 280 million global users in 2024 \cite{AI-users} and projections of up to 1 billion users by 2030. Perhaps the most popular AI models today are large language models (LLMs), which demonstrate impressive language understanding and reasoning capabilities. Alongside them, generative models have emerged, finding applications in music, video, voice, and primarily image generation. These models can create entire complex, high-quality scenes within seconds, thanks to major advances in recent years. Leading examples such as Midjourney \cite{midjourney}, Stable Diffusion \cite{stable-diffusion}, and GPT-4o \cite{GPT-4o} are able to produce photorealistic images as well as artworks in many different styles and forms. They allow people to explore their creativity with few technical limitations, shifting the main challenge toward their effective goal-driven usage.

The most common approach when using image generation models is providing a textual prompt from which the model then generates an image. Other forms of prompts include pictures or sounds, but text remains the most widely used. The problem with the textual approach is its ambiguity: the user does not know how exactly the model will process the prompt or what the main drivers for changes in image output are. More concisely, to the average user, the model behaves like a black box, which makes structured testing to analyze the model's behavior given changing inputs difficult, leading to many trial-and-error attempts that are often too numerous to properly remember or categorize. Consequently, getting the desired result can be challenging and sometimes feel arbitrary.

Many attempts have been made to aid users facing these problems, such as extending and improving multi-modal inputs---for example, allowing users to provide both a text prompt and a rough sketch to an image generation model for more control \cite{lin2025sketchflexfacilitatingspatialsemanticcoherence}. Another promising direction is helping users adjust prompts automatically \cite{10.1145/3729176.3729203,You2024Aesthetic}, as well as hybrid human-in-the-loop approaches that offer both prompt improvement/adjustment and result visualization together with the input \cite{PromptMagician,mishra2025promptaidpromptexplorationperturbation,brade2023promptifytexttoimagegenerationinteractive,guo2024prompthisvisualizingprocessinfluence,promptCharm}.

While existing work focuses on small-scale prompt-image pairs with fine-grained adjustments to understand and improve the prompt-image relationship, we present a tool designed for exploring and understanding what we call \textit{concept drift} in generative models, the different conceptual directions a user or model can take when generating images.

Our approach makes concept drift visible and traceable through:
\begin{itemize}
	\item \textbf{Automated prompt variation:} An LLM-based system that generates prompt alternatives, enabling exploration of the conceptual space.
	\item \textbf{Dual interactive visualization:} Two graphs displaying prompt-image data, a scatter plot displaying images based on similarity, a directed acyclic graph (DAG) displaying the prompt structure 
	\item \textbf{Linked visualization:} We showscase a bidirectional mapping between both graphs enabling users to easily map content of one graph to the other, helping with understanding the conceptual directions the model takes, based on inputs.
\end{itemize}

By providing a holistic overview of how prompt variations systematically affect outputs, our tool helps users understand concept drift patterns and develop more effective prompting strategies. \\

\chapter{Background}
\label{chap:k2}
This section serves as an introduction to topics, concepts and techniques that are relevant to understanding this thesis, introducing the topics of generative models, diffusion models Large-Language-Models (LLMs), prompt engineering, vectorization of images, vector dimensionality reduction, pixel based comparison and visualization techniques. 
\section{Generative models}
%strutcure
%Definition: Learning data distributions to generate novel samples
%Key distinction from discriminative models

%Broad applications: image synthesis, text generation, audio, video

Generative models are a relatively recent phenomenon in the field of AI, they aim to create new unseen data that fits the data they have been trained upon. The most prominent of them are image generation models with the first models capable of generating complex images emerging in 2013 they are called VAE (variational autoencoders) named after the architecture they are based on (Auto-Encoding Variational Bayes) and work by making stochastic sampling differentiable \cite{kingma2022autoencodingvariationalbayes}, however early adaptations resulted in blury and low resolution images.
Then in 2014 goodfellow et. all {goodfellow2014generativeadversarialnetworks} proposed a new process for estimating generative models using an adverserial approach, this is realized by using a two part architecture in training, a generator and a discriminator both neural networks. The generator outputs a sample from noise and the discriminator tries to classify whether that sample is a from a real data distribution or generated based on the result of the discriminator the generators weights are adjusted to result in a less distinguishable output. This approach results in more realistic higher quality images, since the generator is trained to "fool" the discriminator into thinking its output is real. 

In 2017 Vaswani et al pusblished their breakthrough paper \verb|“Attention Is All You Need”| \cite{vaswani2023attentionneed} introducing transformer architecture for AI %self-attention, positional encoding, encoder–decoder structure 
originally intended for sequence-to-sequence tasks their adaptation too generative AI was very successful, with early models using transformer architecture like GPT-1 \cite{GPT-1} and BERT \cite{devlin2019bertpretrainingdeepbidirectional} being quite capable for their respective tasks. However early transformer models for image generation such as Image Transformer \cite{parmar2018imagetransformer} struggeled with producing high quality images, while later attempts and hybrid architecture led to better performance generating images. Then in 2020 Ho et. al published their landmark paper "Denoising Diffusion Probabilistic Models" where they adpated diffusion models for image generation running on transformers.
Subsequent improvements of the approach make diffsion models the state-of-the-art for image generation, which is why they were used for image generation in this work. %add reference to stable diffusion or models being used for live generation

\section{LLMs}
Large-Language-Models (LLMs) have emerged in the field of natural language processing (nlp) but have since transcended their original purpose, most of them function by predicting the next token utilizing a transformer architecture and large scale pre-training on massive amounts of data they are able to achieve impressive results text generation and summarization to translation, reasoning, and dialogue. Their big breakthrough was in 2020 with Brown et al. publishing GPT-3 \cite{brown2020languagemodelsfewshotlearners} first showing  that scaling the number of parameters and training data substantially improves performance. However despite their impressive capabilities LLMs are faced with several drawbacks, they are computationally expensive requiring large resource hungry data centers to train and service request at scale, additionally they may show bias or inaccuracy in their responses depending on the underlying training data, which makes sourcing quality data at the required scale a major challenge LLM developers face. Another negative is their non applicability in safety critical environments due to hallucination, where the model generates plausible but incorrect outputs.

\section{prompt engineering}
In order to use the models introduced in the last sections we need to "prompt" them that means entering a text or an image with instructions for the model based on which some output will be generated. Here what and how we type the prompt is important for the performance of the model as  \cite{zhang2025understandingrelationshippromptsresponse} and \cite{li2024effectsdifferentpromptsquality} have shown for their respective selected tasks. This importance of the prompt has created a whole new field of research called "prompt engineering" where people are trying to find techniques and tricks to improve model output or even bypass certain restrictions. This can be done by hand where humans try to improve prompts through trial-error and evaluation or automatically with some evaluation metric where algorithms or even other machine learning models test through many different prompts and evaluate their performance. This has resulted in several different techniques such as few-shot learning (FSL) \cite{few-shot} where the model is trained to learn a new task on a small (<100) number of labeled examples, the most useful few-shot approach for normal users is in-context learning \cite{in-context-learning} where the users provides examples in the prompt, which the model than uses to infer a pattern which it then applies. \\ Other techniques include Chain-of \cite{chain-of-thought} which aims to improve the reasoning of an LLM by instructing it to show it's reasoning and including examples of intermediate reasoning steps in the prompt. It is especially useful for mathematical problem-solving and commonsense reasoning. Structured output prompting instructs the prompt to provide the response in a formatted way such that is fits for example a json or xml file, this is done to enable smooth parsing of an LLM response and especially useful when integrating the model in a framework or other pieces of automates software. Restricting the output format can negatively impact the performance in some cases especially reasoning tasks, it is however mostly consistent and depending on task and model sometimes even improves. \cite{tam2024letspeakfreelystudy}.

To achieve accurate results some prompt engineering principles were also employed in this thesis.

\section{vectorization of images}
Comparing images is an old and non-trivial problem that is also afflicted by subjectivity, as humans might rate different images as more similar or dissimilar based on individual factors. Human evaluation also takes longer and is more costly which is why a mathematical method is crucial. 
One way is to transform an image into something with many pre-existing comparison methods such as a vector where mathematical similarity measurements are well established. There are several ways to turn an image into a vector such as the "classical" approach where feature extraction is hand-engineered such as SIFT \cite{inbook}, however due to greater generalizability, automatic feature extraction and usually better performance methods using specifically trained machine learning models are generally preferred. Which was also the case in this thesis, where the Img2Vec library \cite{patel2019img2vec} was utilized.

\section{vector dimensonality reduction}
\label{dimRed}
In dimensionality reduction we try to reduce the dimension of a given data points while trying to preserve certain properties and relationships between the data points, in our case we are interested in the similarity of the given data points. I utilized Umap \cite{umap} which builds a nearest neighbour graph utilizing fuzzy sets in the higher dimensional space and then tries to preserve this realtionship in lower dimensional spaces usally 2D or 3D. 
As data pojnts we utilize the vectors we get from the previous section where we try to preserver their similarity while projecting them onto 2d space thus we are able to plot them on a simple 2d graph which is visually one of the most intuitive for humans to grasp. 

\section{pixel based comparison}
Evaluating the similarity between images is an important step in many image processing tasks such as compression, denoising, and enhancement. Traditional measures like Mean Squared Error (MSE) or Peak Signal-to-Noise Ratio (PSNR) compare images directly at the pixel level. While simple and efficient, these methods often do not reflect how humans perceive visual quality. To address this gap, more perceptually similar approaches such as the Structural Similarity Index (SSIM) \cite{ssim} have been introduced. SSIM goes beyond raw pixel differences by incorporating luminance, contrast, and structural information, which makes it better aligned with human visual perception. As a result, SSIM has become a widely used standard for assessing image quality in both research and applied settings.



LaTeX hints are provided in \cref{chap:latexhints}.

%\blinddocument

\chapter{Related Work}
This chapter gives an overview of pre-existing concepts and work related to this thesis 


\section{Prompt Engineering and Visualization}
Mishra et al. (2023) [MSA+25] introduced PromptAid, an interactive system for prompt exploration and iteration when using large language models. Their approach aims to support inexperienced users in creating more effective prompts. Their framework contains three semi-automated strategies: keyword perturbations (replacing or varying keywords), paraphrasing perturbations (rewriting or changing phrasing), and \enquote {selecting an optimal set of in-context few-shot examples}which they aim to visualize for the user. This allows users to easily generate prompt iterations, analyze and compare the outcomes, and iteratively adjust the prompt. \\
  \begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{promptaidInterface.png}
	\caption{prompt aid interface with model selection, explorable prompt space, prompt analysis section, comparison and recommendations. Taken from \cite{mishra2025promptaidpromptexplorationperturbation}}
	\label{fig:promptAid}
\end{figure}

Similarly for image generation models, Brade et al. (2023) \cite{brade2023promptifytexttoimagegenerationinteractive} introduced PromptiFy a framework for prompt suggestion and result visulaziation, they achieve this by utilizing three interrative main methods in their framework 1."automatic prompt extension and suggestion" here the user can first select a general theme for which the framework will query an LLM for suggestions and then an art style, then 2."image layout and clustering by similarity" where the suggested prompts will be send to the underlying model for generation and then displayed in a graph based on their similiarity (the closer the more similar), lastly
 3."automatic prompt refinement suggestions." where an LLM suggest refinements to the prompt to achieve different outcomes. 
 This process again supports the user in finding suitable prompts and understanding the prompt output relationship.\\
  \begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{promptifyInterface.png}
	\caption{ Figure 1 shows the Promptify system interface, which consists of six main components: (A) controls for writing prompts and configuring Stable Diffusion parameters, (B) an automatic suggestion module that provides subject matter ideas and style keywords through natural language steering, (C) an interactive canvas for viewing and organizing generated images with drag-and-drop clustering capabilities, (D) a minimap for navigation and cluster overview, (E) controls for adjusting image positioning and spacing based on similarity, and (F) a prompt history panel for managing previous prompts and their associated images. Image taken from \cite{mishra2025promptaidpromptexplorationperturbation}}
	\label{fig:promptify}
\end{figure}


 Guo et al. (2024) \cite{guo2024prompthisvisualizingprocessinfluence} presented PrompTHis, a visual analytics tool that highlights the process and influence of prompt editing during text-to-image creation. They achieved this by remebering the history of prompt and image pairs and plotting them in a specially designed graph they call an Image Variant Graph. This supports the user in aquiring an understanding of the impact of prompt changes to the image, enabeling them to closer control and influence the generation of the model.\\
 
  \begin{figure}[H]
 	\centering
 	\includegraphics[width=\linewidth]{promptThisInterface.png}
 	\caption{ taken from \cite{guo2024prompthisvisualizingprocessinfluence}}
 	\label{fig:promptThis}
 \end{figure}
 
 
 
 
 
 Promptcharm \cite{promptCharm} takes a hybrid user-in-the-loop framework which offers automated prompt improvement, result visualization while also allowing inpainting and showing and changing model attention of different keywords. Inpainting gives the user the ability to mark undesired areas for the model to change, adjusting the model attention influences how much weight is placed on the keyword influencing the subsequent generation. While their automated prompt improvement leverages Promptist \cite{hao2023optimizingpromptstexttoimagegeneration} helping the user achieve a more aesthetically pleasing result, while enabeling multi viewing of results with different attention.  
 
 \begin{figure}[H]
 	\centering
 	\includegraphics[width=\linewidth]{promptcharm.png}
 	\caption{the promptcharm user interface where you can see the field with the initial prompt the suggested modfied version and 2 outputs with varied attention taken from \cite{promptCharm}}
 	\label{fig:promptCharm}
 \end{figure}
 
 
 Similarly promptMagician \cite{PromptMagician} also uses a prompt improvement then visualization human-in-the-loop approach. They achieve their prompt improvement through a pipeline, which 
 given the initial prompt it automatically generates a collection of image with a range of hyper-parameter, then it searches for similar prompt-image pairs in DiffusionDB \cite{diffusiondb} a prompt-image database. Then they present a summarized visual overview which allows the user to select desired images. After they employ a prompt key word recommendation model on the selection, it clusters similar prompt-images and selects important words from prompts disregarding sentence structure. Then the words are visually matched to their image clusters allowing the user to explore options for prompt adjustment.
 
   
  \begin{figure}[H]
 	\centering
 	\includegraphics[width=\linewidth]{promptMagicianInterface.png}
 	\caption{the prompMagician user interface  taken from \cite{PromptMagician}}
 	\label{fig:promptMagician}
 \end{figure}
 
 
 
 
 
 
  



add figures for promptaid and promptthis?
Describe relevant scientific literature related to your work.
\section{Visualization of image data}
With the recent rise of generative AI came a newfound need of large annotated image collections as well as the novel opportunity of creating large amounts of new never seen before images at the click of a button.\\This resulted in increased interest and need for image data visualization and classification two concepts that often go hand-in-hand as visualizing image collections effectively usually involves classifying or differentiating images in some way to achieve a useful visualization this can range from clustering them by similarity to ordering them based on content (such as perceived emotion). Researches have proposed several techniques to tackle this challenge. Such as treemaps where data is sorted categorically in rectangles, with one rectangle usually containing data of 1 specific category, the graph also allows for rectangles of 2 rectangles allowing to display a parent group, such as cars and trains with their own rectangle (classification) and a larger one containing both categorized as transportation. In some treemaps the size of the individual data points(entries in the rectangles) can vary for image data this is however usually not the case as 1 image generally isn't more important or has a larger value than another. An example of one such adaptation using semantic zooming can be found here \cite{dendromap} \cref{fig:dendroMap}.
 Another method involves displaying the data in scatter plots, a graph with elements strewn in usually a 2D or 3D  space based on some sorting criteria, most scatter plots are similar and normally only differ in the choice of elements (just dots or other symbols, do they have other stuff attached etc.) that are displayed the axis and the ability to zoom or maneuver in the graph. Whether a scatter plot is effective in visualizing data however is not only given by it's appearance, features and design but also largely depends on the underlying computation method which dictates how the data is spatially arranged. One of the most employed method for image data for this right now is dimensionality reduction \cref{dimRed} \cite{CDR,umap}.\\
 In this work we use a modified scatter plot with umap to compute the groupings, this is due to a scatter plot better representing image relations, a grid can't show how dissimilar 2 images are due to the distances being fixed while a scatter plot allows for basically arbitrary distances, as well as wanting to enable the user to get an overview over the entire image space at once, here a scatter plot fits very well for large data amounts, while he is still able to zoom and move within the scatter plot for more precise viewing. 

  \begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{dendroMap.png}
	\caption{example of a word cloud about the US-presidential election 2008  taken from \cite{wordclouds}}
	\label{fig:dendroMap}
\end{figure}




\section{Visualization of large data sets and high dimensional data}
In order to visualize large data sets graphs usually are the go to approach however some things are harder to plot than others, while it might be easy to plot a quarterly profits report plotting high dimensional data like text, videos or images is a different challenge especially when wanting to plot them relative to each other. In our case plotting images based on their similarity, this is challenging due to a lot of traditional measures to determine the similarity of images returning a single number (depending on the implementation usually the closer that number to 0 the similar the images), additionally the similarity is usually measured between 2 images then for larger data sets with possibly hundreds of images we'd have to chose 1 starting image from the data set and compare it all the others to gain our similarity score. This has some potential issues 1. it might not give an accurate representation of the images towards each other when plotted (2 very different images could end up with a very similar difference score to the chosen starting image) making such a measure potentially ineffective for visualizing data. 2. This makes plotting the data in a meaningful way hard as viewing the data on a 1D number line doesn't utilize the full screen space and observes have a harder time to view and relate all the data (as they have to pan from left to right having a larger distance between data points). Also plotting the data in a raster where we go from left to right in descending similarity wouldn't properly quantifying how similar images are  (2 images that are very similar to each other could be next to an image very dissimilar from these first 2), losing that entire dimension of the data in the visualization. \\
Thus research has been done to be able to preserve the similarity relationship while utilizing 2 or 3 dimensions with the first major breakthrough in that direction being t-SNE \cite{van2008visualizing}.

t-Distributed Stochastic Neighbor Embedding \cite{van2008visualizing} is a
nonlinear dimensionality reduction technique that has been widely adopted for
visualizing high-dimensional data. The method models pairwise similarities between
points in both the original space and the low-dimensional embedding as probability
distributions, and seeks to minimize their divergence. The cost function is given by
\begin{equation}
	C = KL(P \,\|\, Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}},
\end{equation}
where $P$ represents similarities in the high-dimensional space and $Q$ those in
the embedding. This approach effectively preserves local neighborhood structure.
However due to its high 
computational cost and limited scalability.
Uniform Manifold Approximation and Projection (UMAP) has been proposed as an
alternative that retains the benefits of t-SNE while offering improved speed and
better preservation of global structure. Making it the prefered choice for larger data sets.


\section{visualization of text content}
Visualizing text content is a complex problem with a multitude of approaches depending on several factors like content size, type and desired result/visualization.\\
Analyzing the most important topics of a document collection with a temporal aspect (showing which topics used to be relevant and which topics are relevant now, in a (over time)changing document collection such as presidential speeches) here one might utilize word clouds where word size and ordering play an important role see \cref{fig:wordClouds}  \cite{wordclouds}  different approach than visualizing a large corpus of scientific papers via an interactive interface with several components including a semantic link graph and a content summary section see \cref{fig:scientificPaperVisualization}\cite{visualizingPaperCollections}. Yet another approach for text documents and sentences is a word tree \cref{fig:wordTree} where words and unique sentence parts are displayed in a tree giving an overview of where and in what context certain words occur these however can get rather large for longer texts or documents \cite{wordTree}. Again others aim to visualize the core content of a social media text collection introducing other challenges and possibilities due to the data being mostly short and unstructured. This allows for a \enquote{variation} of a word tree that utilizes a more lose but still relevant ordering while filtering out more words deemed not relevant enough and displaying more important (usually more often occurring) words larger \cref{fig:sentenTree} \cite{sentenTree}.\\ 
Inspired by the last two approaches we present a variation of a word tree for prompts in form of a dag with the goal of preserving sentence structure for easy similarity assessment while aggregating words and sentence sections as much as possible for a more manageable visual analysis.


  \begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{wordCloud.png}
	\caption{example of a word cloud about the US-presidential election 2008  taken from \cite{wordclouds}}
	\label{fig:wordClouds}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{actionScienceExplorer.png}
	\caption{Interface of action scinece explorer showing the reference management 1-4, a citation statistic 5, the main visualization 6, a section for the citation context (7,9) and a summary section for multiple documents 8   taken from   \cite{visualizingPaperCollections}}
	\label{fig:scientificPaperVisualization}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{wordTree.png}
	\caption{Word tree showing occurrences and subsequent words of i have a dream in the famous speech of martin luther king taken from   \cite{wordTree}}
	\label{fig:wordTree}
	
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{sentenTree.png}
	\caption{example of a sentenTree about social media posts containing yelp and eat24 taken from   \cite{sentenTree}}
	\label{fig:sentenTree}
\end{figure}
\section{Concept Drift in Generative models}
So concept drift = the phenomenon where AI models shift between different conceptual interpretations/outputs based on prompt variations, and your tool is designed to visualize and analyze these shifts systematically.


Traditionally, concept drift in machine learning refers to a change in the mapping between input data and output labels over time. A model is trained on a particular data distribution, but as the relationship between data features and model classification shifts, its predictions become less reliable. For example, a spam email detector trained on data from five years ago may struggle today because spammers use new patterns and tactics that the model never learned, causing it to missclassify more modern spam.\\
In this thesis concept drift refers to the changing the intent of the user while prompting the image generation model. Where in a normal use case it is likely that the prompt will be adjusted altered modified and specified many times in order to achieve the desired end result. This work aims to visualize that process and easy narrowing in on a desired prompt by providing an overview between the image and the prompt space hopefully allowing the user to identify what parts of the image changes based on the changes in the prompt.



where a model has been trained under a certain data set (the training data), but the data it is now applied on has shifted from the training data set, making the model less effective 
\chapter{Approach}
This work aims to highlight the relationship between prompt and generated image in order for the user to better understand how changes in the prompt affect the generated images. \\
A major challenge is visualizing the connection between the prompt and the image especially for many the prompt image pairs that are normally created during the creative process when utilizing an image generation model.\\
For this an interactive approach is chosen  between a visualization of the images and the prompts.\\ Research led to chose a 2D scatter plot for images where circles with the individual image inside are vectorized and  displayed based on their similarity determined by U-map. To display the prompts and allow their easy readability and association with its corresponding prompt a sideways dag-tree was chosen where nodes are computed via the following steps 1. clean up the prompt, removing all special characters numbers and remove articles, pronouns, Auxiliary verbs and some prepositions (of, to, and, or, for) aswell as (with, without, from, by, as) while keeping others (in, at, on) that are deemed to be semantically significant for the image generation. 2. The initial nodes are formed consisting of the word and the position at which it occurs we only look at unique word positions words that appeared already are disregarded though their relationships (parent-child) are recorded, additionally we save whether the word has a child or whether any prompt ends with the word position pair. 3. We merge nodes that only have 1 child and which no prompt ends with. 4. collapse adjacent nodes with same entries 5. Last a couple sweeps with the barycenter heuristic to reduce edge crossings giving the graph a clearer look.\\




\section{dag}
In the following the dag progression for different steps will be shown to visualize the changes. In order for them to be easier to follow only 5 prompts will be chosen. We can see that initially relations between the prompts are hard to glean, even with only 5 prompts comparing the text and identifying the similarities and difference is quite a chore for the human visual complex. Removing stop and function words makes it a little easier due to the text content decreasing however it's still hard and will only exponentially increase in difficulty when adding more prompts (imagine trying to compare 50 or 100 prompts "by hand").\\
Just by aggregating words at the same position and indicating their relation via edges we can already see noticeable improvements, which are only further pronounced when collapsing nodes containing "phrase words", words that appear at that position only next to a relative.\\
However we have lost the crucial information of what nodes belong together with multiple new sentences being possible when just following the graph structure, due to the an edge aggregations followed by an edge split. To remedy this a hover function is introduced where when hovering a node all the node relatives from the original data are shown. This allows for an easier overview and and comparability of the nodes, this approach can scale quite well if prompts are sufficiently similar (if all prompts would differ at every position we'd just have a prompt list again).

\begin{figure}[H]        % 'h' = place here (optional)
	\centering           % centers the image
	\includegraphics[width=\linewidth]{DAGNothingApplied2.png}
	\caption{Raw prompts with no aggregation or processing}
	\label{fig:dagPure}  % optional: for referencing
\end{figure}
\begin{figure}[H]        % 'h' = place here (optional)
	\centering           % centers the image
	\includegraphics[width=\linewidth]{DAGTokenized.png}
	\caption{Prompts with stop and some function words removed}
	\label{fig:dagTokenized}  % optional: for referencing
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth, trim=4cm 1cm 4cm 1cm, clip]{DAGNotCollapsed.png}
	\caption{DAG with nodes at same position merged with edges indicating relationships noticeably reduced size }
	\label{fig:dagMerged}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth, trim=4cm 1cm 4cm 1cm, clip]{DAGCollapsed.png}
	\caption{DAG with nodes collapsed if there are unique to that order and sequence, aswell as neighbourhing nodes collapsed if they have the same content (not the case here so no changes from second collapse)}
	\label{fig:dagCollapsed}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth, trim=4cm 1cm 4cm 1cm, clip]{DAGPathSelected.png}
	\caption{Node selected showing the path (ancestors,descendants) of the node relative to the original input data}
	\label{dagPath}
\end{figure}

\section{scatterplot}
To represent the images a scatterplot was chosen, this was due to 2D representation providing the best overview and being easily understood, while it also helps that humans associate visual closeness with belonging together or to a grouping making the distance between the points being given by similarity being very effective with similar images having a close proximity for an easy overview. Initially the scatter plot only contained dots representing the respective images with a hover functionality displaying the image and prompt. This however didn't give an initial overview over the entire graph which is why instead of just dots the images themselves as small circles were plotted providing the user with a good initial overview over the data. Furthermore zooming and panning is implemented to ease use of the scatterplot and allow the user more control of what he sees and focuses on.
\\Additionally a lens for image selection is introduced the lens can be varied in size allowing for larger or more precise selections, the selection is then highlighted and displayed in a grid next to a cursor. Ideally the images would be ordered based on similarity using for example SSIM as was the case in this work however for live interaction (the user sliding the lens over the scatterplot) SSIM or similar methods are with the current limitations of science and technology to slow, therefore the grid update sorted with a pixel based similarity measure only works in the live application for pre-computed values, which is in contrast to the goal of being able to change the prompt and generate images on the fly then adding the results to the framework immediately.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{scatterplotRawDots.png}
	\caption{ScatterPlot of 100 example iamges being plotted based on similarity }
	\label{fig:scatterDots}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{scatterplotRawHovered.png}
	\caption{ScatterPlot with node hovered (unfortunately cursor doesn't show up in screenshot) }
	\label{fig:scatterHovered}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{scatterplotImages.png}
	\caption{ScatterPlot with images as data indicators  }
	\label{fig:scatterImages}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{gridNoSSIM.png}
	\caption{ScatterPlot with Lens selection displaying grid unsorted }
	\label{fig:scatterNoSSIM}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{gridSSIM.png}
	\caption{ScatterPlot with Lens selection displaying grid sorted with SSIM}
	\label{fig:scatterSSIM}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{gridGeometric.png}
	\caption{ScatterPlot with Lens selection displaying grid with geometirc sorting}
	\label{fig:scatterNoSSIM}
\end{figure}


\section{prompting help}
To assist the user in finding prompts quickly, the interface has a simple prompting tool, where the user can enter a prompt select which parts he'd like to vary aswell as how many variations and how long they should be, then he can select whether he'd like to append or prepend something to the prompt as well whether he'd like to \enquote{mix} all the options (vary every variation option which each other instantly giving a large prompt image space). Then after the user selection and confirmation we build a prompt see \autoref{sec:prompts} with the given inputs where we  LLM called is GPT-5 the newest and most powerful model in the GPT series. In the prompt building we employed prompt engineering techniques namely in-context learning to improve the LLMs accuracy and performance and Structured output prompting in order to be able to be able to send the response to the image generation model directly without further code to handle potential hallucination or formatting issues in post processing \cite{LLMoutputconstraints}, eliminating one avenue of errors or inaccuracy that might be introduced through that. 
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{promptInput2.png}
	\caption{prompt suggestion interface, with A: input area where one can rightclick to toggle the words to vary B: prompt suggestion customization options where one can select how many alternatives and how long they should be, C:history of last used prompts}
	\label{fig:promptInput}
\end{figure}





\section{graph interaction}
A major challenge when trying to visualize results of generative ai in a analytical way is mapping them to their input, this work aims to solve this problem by creating graphs for both and enabling graph interactions in order to map a selection in one graph to the corresponding data in the other, as well as adding a grid displaying selected images for easier viewing increasing the image size relative to the scatter plot elements additionally remedying the issue that with large similar image spaces the images start to significantly overlap thus making the grid necessary for viewing the hidden images.\\
To map from the scatter plot to the dag we chose a lens selection for the images, images selected by the lens are 1.portrait in the grid as aforementioned as and are also mapped to their corresponding words in the dag, by highlighting them while graying out the non related words allowing for a quick visual assessment by the user \cref{fig:scatterDagSelection}.\\
For the complementary selection from dag to scatter plot we chose a simple hover approach, when hovering over a node in the dag that node and all the corresponding elements in the scatter plot are highlighted red while non related elements in both graphs are grayed out \cref{fig:dagScatterSelection}. This again allows for an easy visual mapping of corresponding elements by the user due to color relationship and highlighting as well as graying out relevant and non relevant elements. 


\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{graphInteraction1.png}
	\caption{graph showcasing the interaction between dag and scatter plot}
	\label{fig:graphInteraction}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{scatterDagSelection.png}
	\caption{figure showing selection in scatter plot with matching data in dag}
	\label{fig:scatterDagSelection}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{dagScatterSelection.png}
	\caption{figure showing selection in dag with matching data in scatter plot}
	\label{fig:dagScatterSelection}
\end{figure}

\chapter{implementation}
To realize the chosen design a web application utilizing d3 was implemented
\section{d3}
D3.js (Data-Driven Documents) is a JavaScript library for creating interactive and dynamic data visualizations in web environments. Unlike traditional libraries that provide fixed templates and charts, D3 gives developers fine-grained control over how data is bound to the graphical elements. It utilizes web standards such as HTML, SVG, and CSS, enabling flexible and highly customizable visualizations.

The key strength of D3 is its declarative data binding: datasets can directly drive the creation, transformation, and styling of visual elements. This makes it possible to build anything from simple bar charts to complex, interactive dashboards or exploratory visual analytics systems. Because of this versatility, D3 has become one of the mostcommonly used tools for web-based data visualization in both research and industry.

\section{general framework}
For the general framework a web-application was chosen utilizing html, JS and css for the frontend and python for the backend.
This allows for fast light weight prototyping and easy adpatability. For the generative tasks we used as api call to an image generation model we used a stable diffusion model \cite{stableDiffusion} in this work.
                                                                        
This 
\chapter{use case example}
A user wants to create an image using generative AI for a presentation. However, they are frustrated, during their last generation session, they had to adjust their prompt dozens of times before achieving a satisfactory result. Seeking a more efficient approach, they turn to prompt vision to gain better insight into how prompt modifications affect generated outputs, hoping to reduce time and iterations in future image creation tasks.\\
First the user selects a model \footnote{Model selection is not implemented in the prototype} and enters a prompt in the dedicated field. They really like animals and come up with a scenario for an elephant as well as a quality tag they remember from last time \enquote{A photo of an elephant munching on a stack of hay in a barn, extremely high quality}. They select which words to vary and specify the desired length for alternatives \cref{fig:usecasePrompt1}. Next they choose to let the LLM mix all alternatives with each other curious to explore the different variations. While waiting on the tool response they decide to take a coffee break. 

Upon returning, they are initially surprised by the number of images created, then begin interacting with the DAG visualization by hovering and selecting nodes, aiming to understand how changes in the prompt have affected the images \cref{fig:usecaseQualityDag}. 

After selecting and observing different word combinations, as well as zooming and panning in the scatter plot to better identify images, they note that the differences when using image or photo in the prompt seem minimal, with only drawing looking less real. Next they discover that against their expectations the difference between using fantasy or realistic is quite small. Using cute however drastically changes the output most of the time, making the animals appear more cartoonish with child-like features. Then they observe that the model seems to generate simple poses like sitting well, but fails to show more complex poses, like spying, in the majority of images, and completely fails to display munching. They also notice that the backgrounds are always accurate and the model performs way better when tasked with displaying the apple and the hay stack than the fish. \\
Then the user notes an anomaly in the displayed data: a series of black-and-white images. Thus they use the lens to investigate what part of the prompt could have caused that artifact \cref{fig:usecaseQualityScatter}. Looking at the highlighted words in the DAG they notice that there are multiple options for everything except for drawing, elephant, and barn making the last 3 the likely culprits. They then hover over each option, noticing that 'drawing' has by far the largest portion of black-and-white images, identifying it as the cause.\\
Wanting to experiment with another style they repeat the process with a slightly varied prompt: \enquote{A photo of an elephant munching on a stack of hay in a barn, oil painting} adding the newly generate images to the pre-existing. They compare the new and old images by selecting different word combinations in the DAG. The oil painting style mostly produces more saturated colors and softer edges. However, when examining images tagged with both \emph{realistic} and \emph{oil painting}, they observe that the outputs have more painting qualities while losing photo realism a result of the conflicting descriptors. After exploring further they note that the other previous findings hold true for the newly added images for example fish still generating rarely while the background is always accurate.

 With these newfound insights about how different prompt components affect the generated outputs, they close the tool and proceed to create their presentation image, now better equipped to craft an effective prompt.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{usecasePrompt1.png}
	\caption{Figure showing the prompt field where the user types the prompt and customizes the suggestion opting for 2 alternatives for each selected word each being 1 word long}
	\label{fig:usecasePrompt1}
\end{figure}
\begin{figure}[H]
		\centering
	\includegraphics[width=\linewidth]{usecaseQualityDag.png}
	\caption{Figure showing an example selection in the DAG and the corresponding images in the scatter plot, both highlighted red}
	\label{fig:usecaseQualityDag}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{usecaseQualityScatter.png}
	\caption{Figure showing a lens selection in the scatter plot, the grid displaying the corresponding images in large without overlay and the dag showing the corresponding prompt paths}
	\label{fig:usecaseQualityScatter}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{usecaseQualityPaintingDagZoom.png}
	\caption{Figure showing the selection of \enquote{oil painting} in the dag while being zoomed in on the images}
	\label{fig:usecaseQualityPaintingDagZoom}
\end{figure}


\chapter{conclusion}
\chapter{discussion}
\label{chap:zusfas}
With the approach of visualizing large image-prompt spaces and their relationship in 2 separate interactive graphs, we allow the user to quickly gain an overview over the prompt-image space through interaction with both graphs, as well as adding new prompt-image pairs after making an observation. This enables them to identify how changes in the prompt affect the generated output. From this the user can generalize to the models behavior, struggles with complex poses like sitting, drawing sometimes black and white, background always accurate etc.. The tool is quick and easy to learn however the user needs to make their own observations.\\
Both image data and DAG data need to be sufficiently similar, otherwise this approach would struggle with effectively visualizing the data space. The DAG would be branching to far and growing too large due to aggregation being scarcely possible if the words are too different and the scatter plot would be scattered displaying a bunch of individual dissimilar points with insufficient clustering or similarity sorting.\\
For a large image space (>1000) the tool struggles with long response time from the image generation model, and with older hardware significant lag in d3s zooming and pan behavior. The scatter plot becomes quite growded making the lens selection and grid necessary to identify otherwise over layered images.\\
Compared to other works like \cite{PromptMagician}, we focus solely on a comparatively large prompt-image space, lacking options to customize model hyper parameters and model attention. This makes the tool initially easier to use but allows for less possibilities.


Our approach, which visualizes the prompt-image space through two linked interactive graphs, enables users to efficiently explore the relationships between text prompts and their corresponding generated images. By interacting with both the scatter plot and the directed text-DAG, and iteratively as well as adding new prompt-image pairs based on their observations, users can quickly identify how specific prompt modifications influence the visual output. This process facilitates the generalization of the model's underlying behavior, allowing users to deduce its common tendencies such as a struggle with complex poses (e.g., "munching"), a tendency to generate in multiple styles given certain modifiers (e.g. black-and-white as well as color images for "drawing"), and a consistent accuracy in rendering backgrounds. These insights have practical value for prompt engineering especially for novice users. Understanding which prompt components reliably affect outputs versus which produce inconsistent results allows users to craft more effective prompts with fewer iterations. Rather than trial-and-error experimentation, users can develop systematic strategies based on observed patterns.

The effectiveness of this method depends on sufficiently similar data. If the prompt-image pairs are too dissimilar, the DAG would fail to aggregate nodes meaningfully, resulting in excessive branching and an unwieldy structure. Similarly, the scatterplot would devolve into a disorganized set of points lacking meaningful clusters, thereby hindering the discovery of patterns. This is not a concern when the user uses the LLM-prompt-suggestion feature can however become one when adding many new dissimilar prompts.

A technical limitation of our tool is its scalability. For large datasets exceeding 1,000 images, two significant challenges arise: long waits for responses from the image generation model and significant latency for the D3.js zooming and panning interactions. Furthermore, a crowded scatter plot necessitates the use of the lens and grid selection tools to resolve overlapping images, which adds a layer of interaction complexity.

Our work occupies a distinct position in the landscape of prompt visualization tools. While systems like PromptMagician \cite{PromptMagician} provide granular control over model hyperparameters and attention mechanisms with a focus on clustered representative images, our tool prioritizes breadth of exploration over depth of control. This design choice reflects our goal of supporting discovery and understanding rather than fine-tuned generation.
This trade-off has advantages: Lower cognitive load makes the tool accessible for users exploring unfamiliar models or beginning prompt engineering. The comprehensive view of the prompt-image space reveals patterns that might be missed when focusing on a few representative examples. However, this comes at the cost of advanced customization—users seeking precise control over attention weights or specific model parameters would benefit more from tools like PromptMagician.















\section{Outlook}
Include hyper parameters, and model attention (for attention potentially vary word size in dag based on attention given to each word, hard for multiple prompts with different attention distribution, could be solved for single prompt by showing dag and attention sizes on hover of single image), try to add them in visualization, make images have wider variety, Improve ve response time (model and hardware issue),  allow user possibility to pick prompt alternatives.
Divide an conquer approach allow free selection of images and display them in smaller scatter plot and dag if desired by user

\printbibliography

All links were last followed in October 2025.

\appendix
%\input{latexhints-english}
\chapter{appendix}
\section{prompts}
\label{sec:prompts}
This section shows how the code for the prompt creation in python based on user input.
Prompt for normal varying:
\begin{verbatim}
	if prepend and append:
	extra_instruction = (
	f"Prepend and append {lengthPrependAppend} words to each prompt. "
	f"These can range from setting to style to image clarity and consistency improvements. "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	elif prepend:
	extra_instruction = (
	f"Prepend {lengthPrependAppend} words to each prompt. "
	f"These can range from setting to style to image clarity and consistency improvements. "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	elif append:
	extra_instruction = (
	f"Append {lengthPrependAppend} words to each prompt. "
	f"These can range from setting to style to image clarity and consistency improvements. "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	
	
	
	examples = """
	Example 1:
	Input prompt: "a cat sitting on a windowsill"
	Words to vary: "cat, sitting"
	Number of alternatives: 2
	Prepend 3 words
	
	Output:
	{
		"prompts": [
		"a cat sitting on a windowsill",
		"Golden hour lighting a tabby resting on a windowsill",
		"Photorealistic digital art a kitten perched on a windowsill"
		]
	}
	
	Example 2:
	Input prompt: "futuristic cityscape"
	Words to vary: "futuristic"
	Number of alternatives: 2
	Append 4 words
	
	Output:
	{
		"prompts": [
		"futuristic cityscape",
		"cyberpunk cityscape, neon lights, rain-soaked streets",
		"sci-fi cityscape, highly detailed, octane render"
		]
	}
	
	Example 3:
	Input prompt: "mountain landscape at sunset"
	Words to vary: "mountain, sunset"
	Number of alternatives: 2
	Prepend 2 and append 2 words
	
	Output:
	{
		"prompts": [
		"mountain landscape at sunset",
		"Oil painting alpine peaks at dusk, warm colors",
		"Dramatic composition rocky cliffs at twilight, cinematic lighting"
		]
	}
	"""
	
	user_prompt = (
	f"{examples}\n\n"
	f"Now apply the same approach:\n"
	f"Input prompt: {prompt}\n"
	f"Words to vary: {wordsToChange}\n"
	f"Number of alternatives: {numAlternatives}\n"
	f"Alternative length: {alternativeLengthWhenWordSelected}\n"
	f"{extra_instruction}\n\n"
	f"Generate the original prompt plus {numAlternatives} varied alternatives following the pattern shown above and return the output in a Json format where each promt is a new entry."
	)
\end{verbatim}
Prompt for mixing all variations: 
\begin{verbatim}
	Inhalextra_instruction = ""
	if prepend and append:
	extra_instruction = (
	f"Prepend and append {lengthPrependAppend} words to each prompt. Treat these the same as 'words to vary'"
	f"These can range from setting to style to image clarity and consistency improvements.  "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	elif prepend:
	extra_instruction = (
	f"Prepend {lengthPrependAppend} words to each prompt. Treat these the same as 'words to vary' "
	f"These can range from setting to style to image clarity and consistency improvements. "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	elif append:
	extra_instruction = (
	f"Append {lengthPrependAppend} words to each prompt. Treat these the same as 'words to vary' "
	f"These can range from setting to style to image clarity and consistency improvements. "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	
	examples = """
	Example 1:
	Input prompt: "a grey cat sitting on the floor"
	Words to vary: ["grey", "cat"]
	Alternatives per word: 3
	
	Step 1 - Generate alternatives:
	- "grey" → ["orange", "black", "brown"]
	- "cat" → ["kitten", "dog", "persian"]
	
	Step 2 - Generate all combinations:
	{
		"word_variations": {
			"grey": ["orange", "black", "brown"],
			"cat": ["kitten", "dog", "persian"]
		},
		"combined_prompts": [
		"a orange kitten sitting on the floor",
		"a orange dog sitting on the floor",
		"a orange persian sitting on the floor",
		"a black kitten sitting on the floor",
		"a black dog sitting on the floor",
		"a black persian sitting on the floor",
		"a brown kitten sitting on the floor",
		"a brown dog sitting on the floor",
		"a brown persian sitting on the floor"
		]
	}
	
	Example 2:
	Input prompt: "sunset over mountains"
	Words to vary: ["sunset", "mountains"]
	Alternatives per word: 2
	
	Step 1 - Generate alternatives:
	- "sunset" → ["dawn", "twilight"]
	- "mountains" → ["hills", "peaks"]
	
	Step 2 - Generate all combinations:
	{
		"word_variations": {
			"sunset": ["dawn", "twilight"],
			"mountains": ["hills", "peaks"]
		},
		"combined_prompts": [
		"dawn over hills",
		"dawn over peaks",
		"twilight over hills",
		"twilight over peaks"
		]
	}
	
	Example 3:
	Input prompt: "a red sports car on the highway"
	Words to vary: ["red", "sports car"]
	Alternatives per word: 2
	
	Step 1 - Generate alternatives:
	- "red" → ["blue", "silver"]
	- "sports car" → ["sedan", "SUV"]
	
	Step 2 - Generate all combinations:
	{
		"word_variations": {
			"red": ["blue", "silver"],
			"sports car": ["sedan", "SUV"]
		},
		"combined_prompts": [
		"a blue sedan on the highway",
		"a blue SUV on the highway",
		"a silver sedan on the highway",
		"a silver SUV on the highway"
		]
	}
	"""t...
\end{verbatim}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{graphInteractionLong.png}
	\caption{graph showing detailed graph interactions}
	\label{fig:GraphInteractionLong}
\end{figure}
\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Versicherung
\end{document}
