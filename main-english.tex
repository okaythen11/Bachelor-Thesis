% !TeX document-id = {4719ec8f-70f7-4005-9222-7d90db5fa2e2}
% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-


% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iffalse
\let\ifenglisch\iftrue
\input{pre-documentclass}
\documentclass[
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-sided printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}


\usepackage[
  title={Visual analysis of concept drift in generative models},
  author={Oliver Seiz},
  type=bachelor,
  institute={Institute for Visualization and Interactive Systems\\
  University of Stuttgart\\
  Universitätsstraße 38\\
  D–70569 Stuttgart}, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Informatik},
  examiner={Dr. Steffen Koch},
  supervisor={Jena Satkunaranjan},
  startdate={April 08, 2025},
  enddate={Oktober 13, 2025}
]{scientific-thesis-cover}

\input{acronyms}

\makeindex

\begin{document}

%tex4ht-Konvertierung verschönern
\iftex4ht
  % tell tex4ht to create pictures also for formulas starting with '$'
  % WARNING: a tex4ht run now takes forever!
  \Configure{$}{\PicMath}{\EndPicMath}{}
  %$ % <- syntax highlighting fix for emacs
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
\pagenumbering{arabic}
\Titelblatt

%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftriplepagestyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftriplepagestyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}



%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis
\section*{Abstract}

	Text-to-image generative models can produce high-quality images from textual prompts, but understanding how prompt modifications affect outputs remains challenging. Small changes can trigger dramatic conceptual shifts while seemingly significant modifications may have minimal impact. Normal users are left with a trial-and-error approach to make sense of conceptual shifts, lacking a holistic overview of conceptual directions. 
	
	We present Prompt Vision, a visual-interactive system to explore concept drift in text-to-image generation. Our approach combines automated LLM-based prompt variation with dual linked visualizations: a scatter plot showing semantic organized based on image similarity and a directed acyclic graph displaying the prompt structure. This enables users to systematically explore large prompt-image spaces (400+ variations) and trace visual patterns back to specific prompt components.
	
	Through a detailed use case with Stable Diffusion, we demonstrate that users can identify systematic model behaviors—such as reliable rendering of backgrounds but struggles with complex actions—and develop more effective prompting strategies. Our work shows that visualizing concept drift makes the prompt-image relationship more transparent and controllable, reducing reliance on trial-and-error and supporting more systematic exploration of generative AI capabilities.



\cleardoublepage

%Solely for German courses of study


\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

% Bei einem ungünstigen Seitenumbruch im Inhaltsverzeichnis, kann dieser mit
% \addtocontents{toc}{\protect\newpage}
% an der passenden Stelle im Fließtext erzwungen werden.




%Wird nur bei Verwendung von der lstlisting-Umgebung mit dem "caption"-Parameter benoetigt
%\lstlistoflistings
%ansonsten:


%mittels \newfloat wurde die Algorithmus-Gleitumgebung definiert.
%Mit folgendem Befehl werden alle floats dieses Typs ausgegeben
 %Ist nur für Algorithmen, die mittels \begin{algorithm} umschlossen werden, nötig

% Abkürzungsverzeichnis
\printnoidxglossaries

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Introduction}
\label{sec:introduction}
With the growing capabilities of artificial intelligence (AI) models in recent years, their usage has become more widely adopted, with some sources claiming 280 million global users in 2024 \cite{AI-users} and projections of up to 1 billion users by 2030. Perhaps the most popular AI models today are large language models (LLMs), which demonstrate impressive language understanding and reasoning capabilities. Alongside them, generative models have emerged, finding applications in music, video, voice, and primarily image generation. These models can create entire complex, high-quality scenes within seconds, thanks to major advances in recent years. Leading examples such as Midjourney \cite{midjourney}, Stable Diffusion \cite{stable-diffusion}, and GPT-4o \cite{GPT-4o} are able to produce photorealistic images as well as artworks in many different styles and forms. They allow people to explore their creativity with few technical limitations, shifting the main challenge toward their effective goal-driven usage.

The most common way to use an image generation models is providing a textual prompt in natural language from which the model then generates an image. Other forms of prompts may include pictures or sounds, but text remains the most widely used. The problem with using text prompts in the generation process is ambiguity: the user does not know how exactly the model will process the prompt or what the main drivers for changes in the output image are. More concisely, to the average user, the model behaves like a black box. This makes structured testing, to analyze the model's behavior given changing inputs, difficult leading to many trial-and-error attempts that are often too numerous to properly remember or categorize. Consequently, getting the desired result can be challenging and sometimes feel arbitrary.

Many attempts have been made to aid users facing these problems, such as extending and improving multi-modal inputs---for example, allowing users to provide both a text prompt and a rough sketch to an image generation model for more control \cite{lin2025sketchflexfacilitatingspatialsemanticcoherence}. Another promising direction is helping users adjust prompts automatically \cite{10.1145/3729176.3729203,You2024Aesthetic}, as well as hybrid human-in-the-loop approaches that offer both prompt improvement/adjustment and result visualization together with the input \cite{PromptMagician,mishra2025promptaidpromptexplorationperturbation,brade2023promptifytexttoimagegenerationinteractive,guo2024prompthisvisualizingprocessinfluence,promptCharm}.

While existing work focuses on small-scale prompt-image pairs with fine-grained adjustments to understand and improve the prompt-image relationship, we present a tool designed for exploring and understanding what we call \textit{concept drift} in generative models, the different conceptual directions a user or model can take when generating images.

Our approach makes concept drift visible and traceable through:
\begin{itemize}
	\item \textbf{Automated prompt variation:} An LLM-based system that generates prompt alternatives, enabling exploration of the conceptual space.
	\item \textbf{Dual interactive visualization:} Two graphs displaying prompt-image data, a scatter plot displaying images based on similarity, a directed acyclic graph (DAG) displaying the prompt structure 
	\item \textbf{Linked visualization:} We showscase a bidirectional mapping between both graphs enabling users to easily map content of one graph to the other, helping with understanding the conceptual directions the model takes, based on inputs.
\end{itemize}

By providing a holistic overview of how prompt variations systematically affect outputs, our tool helps users understand concept drift patterns and develop more effective prompting strategies. \\
Beginning with chapter 2, this part has the goal of providing the necessary understanding and motivation for the thesis. In chapter 3 we go over pre-existing approaches similar to our work as well as related key concepts. Chapter 4 details the chosen approach and design goals/decisions while giving an overview and explanation of the frameworks components. Chapter 5 gives a detailed use case describing how one may use our framework and what findings they could infer. Chapter 6 discusses our approach. Chapter 7 concludes the paper.

\chapter{Background}
\label{chap:k2}
This section serves as an introduction to topics, concepts and techniques that are relevant to understanding this thesis, introducing the topics of generative models, diffusion models Large-Language-Models (LLMs), prompt engineering, vectorization of images, vector dimensionality reduction and pixel based comparison
\section{Generative models}
%strutcure
%Definition: Learning data distributions to generate novel samples
%Key distinction from discriminative models

%Broad applications: image synthesis, text generation, audio, video



Generative models are a class of AI models that aim to create new, previously unseen data fitting the distribution of their training data. The most prominent application is image generation, where models learn to produce realistic or artistic images from various input modalities.

\subsubsection{Early Approaches: VAEs and GANs}

The first models capable of generating complex images emerged in 2013 with Variational Autoencoders (VAEs), named after their underlying architecture (Auto-Encoding Variational Bayes). VAEs work by making stochastic sampling differentiable \cite{kingma2022autoencodingvariationalbayes}, enabling gradient-based optimization of generative processes. However, early implementations resulted in blurry, low-resolution images, limiting their practical applications.

In 2014, Goodfellow et al.\ \cite{goodfellow2014generativeadversarialnetworks} proposed a new approach for training generative models using an adversarial process. Generative Adversarial Networks (GANs) use a two-part training architecture: a generator and a discriminator, both neural networks. The generator produces samples from random noise, while the discriminator attempts to classify whether each sample comes from the real data distribution or is generated. Based on the discriminator's feedback, the generator's weights are adjusted to produce less distinguishable outputs. This adversarial training approach results in more realistic, higher-quality images, since the generator is trained to ``fool'' the discriminator into believing its outputs are real. GANs significantly improved image quality compared to VAEs but suffered from training instability and mode collapse issues.

\subsubsection{Transformer Architecture and Its Impact}

In 2017, Vaswani et al.\ published their breakthrough paper ``Attention Is All You Need'' \cite{vaswani2023attentionneed}, introducing the transformer architecture, originally designed for sequence-to-sequence tasks in natural language processing. Transformers use self-attention mechanisms to process input sequences, enabling parallel computation and better capturing of long-range dependencies compared to recurrent neural networks. Their adaptation to generative AI proved highly successful, with early transformer-based models like GPT-1 \cite{GPT-1} and BERT \cite{devlin2019bertpretrainingdeepbidirectional} demonstrating strong capabilities for their respective tasks.

However, early attempts to apply transformers directly to image generation, such as Image Transformer \cite{parmar2018imagetransformer}, struggled to produce high-quality images due to the computational complexity of processing high-dimensional image data. Later hybrid architectures combining transformers with convolutional approaches achieved better performance.

\subsubsection{Diffusion Models: Current State-of-the-Art}

In 2020, Ho et al.\ published their landmark paper ``Denoising Diffusion Probabilistic Models'' \cite{ho2020denoisingdiffusionprobabilisticmodels}, introducing a fundamentally different approach to image generation. Diffusion models work by learning to reverse a gradual noising process: during training, they learn to remove noise added to real images in small steps; during generation, they start with pure noise and iteratively denoise it into a coherent image. This process is guided by text embeddings (in text-to-image models) that condition the denoising steps.

Subsequent improvements, including latent diffusion models \cite{rombach2022highresolutionimagesynthesislatent} which operate in a compressed latent space rather than pixel space, have made diffusion models the current state-of-the-art for image generation. These models power popular systems like Stable Diffusion \cite{stable-diffusion}, Midjourney \cite{midjourney}, and DALL-E, capable of generating photorealistic images and diverse artistic styles from natural language descriptions.\\
The focus of this work lies in evaluating input-output relationships of generative models, we used Stable Diffusion, however our approach is model independent as the model origin of the prompt-image pairs is not relevant to it.





\subsection{Large Language Models}

Large Language Models (LLMs) are neural networks trained on massive amounts of text data, to predict the next token in a sequence. Most modern LLMs utilize transformer architectures \cite{vaswani2023attentionneed} and achieve impressive performance across diverse natural language processing tasks, including text generation, summarization, translation, reasoning, and dialogue.

Their breakthrough came in 2020 when Brown et al.\ published GPT-3 \cite{brown2020languagemodelsfewshotlearners}, demonstrating that substantially scaling both model parameters and training data improves performance across tasks. This scaling paradigm has driven the development of increasingly larger and more capable models like GPT-5 \cite{GPT-4o}, Claude\cite{anthropic2024claude}, and open-source alternatives such as LLaMA \cite{touvron2023llama}.

Despite their impressive capabilities, LLMs face several challenges. They are computationally expensive, require large resource-intensive data centers for both training and inference at scale and may exhibit biases or inaccuracies reflecting their training data, making high-quality data curation a major challenge for LLM developers. Additionally, LLMs are prone to \textit{hallucination}---generating plausible but factually incorrect outputs---which limits their applicability in safety-critical environments.
In this work we use GPT-5 to generate prompt alternatives.






\section{Prompt Engineering}
In order to use generative models users need to provide a prompt, a form of input guiding the generation process of the model. This prompt is usually in natural language but can also be an image video or sound depending on the model and application.\\
The quality and structure of prompts significantly impact model performance \cite{zhang2025understandingrelationshippromptsresponse,li2024effectsdifferentpromptsquality}, this makes prompt design a critical skill for effective model usage. And spawned a whole new field of research called \enquote{Prompt Engineering}.
Prompt engineering refers to the practice of crafting effective usually textual inputs to guide generative models toward desired outputs. In this work, prompt engineering appears in two contexts, we try to aid the user in crafting more effective text-to-image prompts through visualizations and we use LLMs to craft alternative prompts from the initial user prompt.

\subsection{Prompt Engineering for Text-to-Image Models}

Effective text-to-image prompts should describe visual concepts, compositions, and stylistic attributes \cite{oppenlaender2023taxonomy,liu2022design}. They typically contain subject terms, style modifiers, quality boosters, and composition guidance. Research shows that function words, word ordering and punctuation do not contribute significantly to the output quality \cite{liu2022design}. Prompts are however model-specific thus behavior may vary from model to model. Additionally the stochastic nature of models makes it difficult to reliably predict the effects of word choices \cite{liu2022design,hao2022optimizing}.

\subsection{Prompt Engineering for Language Models}

When using language models to generate content or perform tasks, prompt engineering involves providing clear instructions, context, examples and encouraging reasoning. Several established techniques are commonly used improve LLM performance:

\textbf{Few-shot-learning} \cite{few-shot} few-shot learning (FSL) is a technique where the model is trained to learn a new task on a small (<100) number of labeled examples, the most useful few-shot approach for normal users is in-context learning \cite{in-context-learning} where the users provides examples in the prompt, which the model uses to infer a pattern,  which it applies to new inputs.

\textbf{Chain-of-thought prompting} \cite{chain-of-thought} instructs the model to show its reasoning process, improving performance on reasoning tasks by encouraging intermediate reasoning steps in the prompt. And is primarily used for mathematical and logical problems.

\textbf{Structured output prompting} instructs the model to format responses in specific ways (e.g., JSON, XML), enabling automated parsing. This is essential when integrating LLMs into larger systems. While output constraints can sometimes impact performance on reasoning tasks, they are generally consistent and sometimes even beneficial for generation tasks \cite{tam2024letspeakfreelystudy}.

\subsection{Approaches to Prompt Engineering}

Prompt engineering can be performed manually, automatically, or through hybrid approaches:

\textbf{Manual approaches} rely on human iteration: users modify prompts based on outputs, gradually refining toward desired results through trial and error. This process can be time-consuming and requires users to remember which modifications were effective.

\textbf{Automated approaches} use algorithms or other models to generate and evaluate prompt variations. Recent work has explored using LLMs for prompt refinement \cite{10.1145/3729176.3729203,You2024Aesthetic}, leveraging their language understanding to generate improved alternatives.

\textbf{Hybrid approaches} combine human guidance with automated generation, allowing users to specify which aspects to vary while the system generates alternatives \cite{PromptMagician,mishra2025promptaidpromptexplorationperturbation,brade2023promptifytexttoimagegenerationinteractive}. These approaches balance user control with computational efficiency.





\section{Vectorization of images}
\label{sec:vecImg}
Comparing images is an old and non-trivial problem that is also afflicted by subjectivity, as humans might rate different images as more similar or dissimilar based on individual factors. Human evaluation also takes longer and is more costly which is why a mathematical (computational) method is crucial. 
One way is to transform an image into something with many pre-existing comparison methods such as a vector where mathematical similarity measurements are well established. There are several ways to turn an image into a vector such as the "classical" approach where feature extraction is hand-engineered such as SIFT \cite{inbook}, however due to greater generalizability, automatic feature extraction and usually better performance methods using specifically trained machine learning models are generally preferred.\\ Performance in the vectorization of images is generally measured by how similar vectors are for semantically or structurally similar images. Since similarity image similarity is a key part of our vectorization we opted for the model approach utilizing the Img2Vec library \cite{patel2019img2vec}.

\section{Vector dimensonality reduction}
\label{dimRed}
In dimensionality reduction we try to reduce the dimension of given data points while trying to preserve certain properties and relationships between them. In our case we are mainly interested in the similarity of vectorized images. Thus we utilized Umap \cite{umap} which builds a nearest neighbour graph utilizing fuzzy sets in the higher dimensional space and then tries to preserve this realtionship in lower dimensional spaces usally 2D or 3D. 


\section{Pixel based comparison}
\label{sec:pixComp}
Evaluating the similarity between images is an important step in many image processing tasks such as compression, denoising, and enhancement. Traditional measures like Mean Squared Error (MSE) \cite{wang2009mse} or Peak Signal-to-Noise Ratio (PSNR) \cite{hore2010psnr} compare images directly at the pixel level. While simple and efficient, these methods often do not reflect how humans perceive visual quality. To address this gap, more perceptually similar approaches such as the Structural Similarity Index (SSIM) \cite{ssim} have been introduced. SSIM goes beyond raw pixel differences by incorporating luminance, contrast, and structural information, which makes it better aligned with human visual perception. As a result, SSIM has become a widely used standard for assessing image quality in both research and applied settings. Initially we sought to use it for displaying scatter plot selection in the grid but later we will show why this wasn't possible.




%\blinddocument

\chapter{Related Work}
This chapter gives an overview of pre-existing concepts and work related to this thesis 


\section{Prompt Engineering through Visualization}
Mishra et al. (2023) [MSA+25] introduced PromptAid, an interactive system for prompt exploration and iteration when using large language models. Their approach aims to support inexperienced users in creating more effective prompts. Their framework contains three semi-automated strategies: keyword perturbations (replacing or varying keywords), paraphrasing perturbations (rewriting or changing phrasing), and \enquote {selecting an optimal set of in-context few-shot examples}which they aim to visualize for the user. This allows users to easily generate prompt iterations, analyze and compare the outcomes, and iteratively adjust the prompt. \\
  \begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{promptaidInterface.png}
	\caption{prompt aid interface with model selection, explorable prompt space, prompt analysis section, comparison and recommendations. Taken from \cite{mishra2025promptaidpromptexplorationperturbation}}
	\label{fig:promptAid}
\end{figure}

Similarly for image generation models, Brade et al. (2023) \cite{brade2023promptifytexttoimagegenerationinteractive} introduced PromptiFy a framework for prompt suggestion and result visulaziation, they achieve this by utilizing three interrative main methods in their framework 1."automatic prompt extension and suggestion" here the user can first select a general theme for which the framework will query an LLM for suggestions and then an art style, then 2."image layout and clustering by similarity" where the suggested prompts will be send to the underlying model for generation and then displayed in a graph based on their similiarity (the closer the more similar), lastly
 3."automatic prompt refinement suggestions." where an LLM suggest refinements to the prompt to achieve different outcomes. 
 This process again supports the user in finding suitable prompts and understanding the prompt output relationship.\\
  \begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{promptifyInterface.png}
	\caption{ Figure 1 shows the Promptify system interface, which consists of six main components: (A) controls for writing prompts and configuring Stable Diffusion parameters, (B) an automatic suggestion module that provides subject matter ideas and style keywords through natural language steering, (C) an interactive canvas for viewing and organizing generated images with drag-and-drop clustering capabilities, (D) a minimap for navigation and cluster overview, (E) controls for adjusting image positioning and spacing based on similarity, and (F) a prompt history panel for managing previous prompts and their associated images. Image taken from \cite{mishra2025promptaidpromptexplorationperturbation}}
	\label{fig:promptify}
\end{figure}


 Guo et al. (2024) \cite{guo2024prompthisvisualizingprocessinfluence} presented PrompTHis, a visual analytics tool that highlights the process and influence of prompt editing during text-to-image creation. They achieved this by remebering the history of prompt and image pairs and plotting them in a specially designed graph they call an Image Variant Graph. This supports the user in aquiring an understanding of the impact of prompt changes to the image, enabeling them to closer control and influence the generation of the model.\\
 
  \begin{figure}[H]
 	\centering
 	\includegraphics[width=\linewidth]{promptThisInterface.png}
 	\caption{ Figure showing the promptThis user interface with a) showing their image prompt graph here we can see the history of the prompts, different images and corresponding annotations that mark what has been added or removed are shown b) shows a minimap helping the user navigate prompts and their outputs c) is a control panel  d) shows the customizations one can make during creation e) shows modifiers utilized in previous prompts taken from \cite{guo2024prompthisvisualizingprocessinfluence}}
 	\label{fig:promptThis}
 \end{figure}
 
 
 
 
 
 Promptcharm \cite{promptCharm} takes a hybrid user-in-the-loop framework which offers automated prompt improvement, result visualization while also allowing inpainting and showing and changing model attention of different keywords. Inpainting gives the user the ability to mark undesired areas for the model to change, adjusting the model attention influences how much weight is placed on the keyword influencing the subsequent generation. While their automated prompt improvement leverages Promptist \cite{hao2023optimizingpromptstexttoimagegeneration} helping the user achieve a more aesthetically pleasing result, while enabling multi viewing of results with different attention.  
 
 \begin{figure}[H]
 	\centering
 	\includegraphics[width=\linewidth]{promptcharm.png}
 	\caption{the promptcharm user interface where you can see the field with the initial prompt the suggested modfied version and 2 outputs with varied attention taken from \cite{promptCharm}}
 	\label{fig:promptCharm}
 \end{figure}
 
 
 Similarly promptMagician \cite{PromptMagician} also uses a prompt improvement then visualization human-in-the-loop approach. They achieve their prompt improvement through a pipeline, which 
 given the initial prompt it automatically generates a collection of image with a range of hyper-parameter, then it searches for similar prompt-image pairs in DiffusionDB \cite{diffusiondb} a prompt-image database. Then they present a summarized visual overview which allows the user to select desired images. After they employ a prompt key word recommendation model on the selection, it clusters similar prompt-images and selects important words from prompts disregarding sentence structure. Then the words are visually matched to their image clusters allowing the user to explore options for prompt adjustment.
 
   
  \begin{figure}[H]
 	\centering
 	\includegraphics[width=\linewidth]{promptMagicianInterface.png}
 	\caption{the prompMagician user interface showing a) the prompt input and generation customization field b) the iamge browser displaying different images with distinct styles c)the image evaluation field evaluating a selected image based on chosen criteria d) a dfield showing local explorations by varying specific parts of the prompt  taken from \cite{PromptMagician}}
 	\label{fig:promptMagician}
 \end{figure}
 
 
 
 
 
 
  




\section{Visualizing Image Data}
With the recent rise of generative AI came a newfound need of large annotated image collections as well as the novel opportunity of creating large amounts of new never seen before images at the click of a button.\\This resulted in increased interest and need for image data visualization and classification two concepts that often go hand-in-hand as visualizing image collections effectively usually involves classifying or differentiating images in some meaningful way. This can range from clustering them by similarity to ordering them based on content, such as perceived emotion or animal categories. However displaying results in a human coherent way is a problem that increases dramatically with growing data sizes. Researches have proposed several techniques to tackle this challenge. Such as tree maps where data is sorted categorically in rectangles, with one rectangle usually containing data of 1 specific category. The graph also allows for rectangles to contain other rectangles allowing for the display a \enquote{belongs-to} relationship, such as cars and trains with their respective rectangle being encompased by the rectangle for transportation. In some treemaps the size of the individual data points(entries in the rectangles) can vary for image data this is however usually not the case as one image generally isn't more important or has a larger value than another. An example of a tree-map adaptation for images that uses semantic zooming can be found here \cite{dendromap} \cref{fig:dendroMap}.
 Other methods involves displaying the data in scatter plots, a graph with elements strewn in usually a 2D or 3D  space based on some sorting criteria, most scatter plots are similar and normally only differ in the choice of elements (just dots or other symbols, do they have other stuff attached etc.) that are displayed the axis and the ability to zoom or maneuver in the graph. Whether a scatter plot is effective in visualizing data however is not only given by it's appearance, features and design but also largely depends on the underlying computation method which dictates how the data is spatially arranged. One of the most employed method for image data for this right now is dimensionality reduction \cref{dimRed} \cite{CDR,umap}.\\
 In this work we use a modified scatter plot with UMAP to compute the groupings, this is due to a scatter plot better representing image relations, a grid can't show how dissimilar 2 images are due to the distances being fixed while a scatter plot allows for basically arbitrary distances, as well as wanting to enable the user to get an overview over the entire image space at once, here a scatter plot fits very well for large data amounts, while he is still able to zoom and move within the scatter plot for more precise viewing. 

  \begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{dendroMap.png}
	\caption{example of a tree map adapted for displaying images we can see the different rectangles containing images (these images were chosen to represent a lost more images as indicated by the number at the top left of each rectangle) of e.g. cats and birds while being encompassed by a greater rectangle for the category of animals taken from \cite{wordclouds}}
	\label{fig:dendroMap}
\end{figure}




\section{Visualization of Large Data Sets of High Dimensional Data}
\label{sec:HighData}
Visualizing large datasets is often best accomplished with graphs, but some data types present more of a challenge than others. While plotting a quarterly profits report is straightforward, plotting high-dimensional data like text or images is difficult. This is especially true when the goal is to show how data points relate to one another.

In our case, we want to plot images based on their similarity. This is challenging for two main reasons. First, traditional image similarity measures often return a single numerical score (where a value closer to zero typically indicates higher similarity). Second, this score is usually calculated between just two images. For a larger dataset, this means selecting a single starting image and comparing all others to it. This approach has significant drawbacks:

\begin{enumerate}
	\item \textbf{Inaccurate Representation:} It may not accurately represent the relationships between all images. Two very different images could end up with a similar score to the chosen starting image, making the measure ineffective for visualization.
	
	\item \textbf{Poor Use of Space:} Plotting the data on a one-dimensional number line fails to utilize the available screen space. It also forces observers to pan horizontally, making it difficult to view and relate all the data points at once. Furthermore, a simple raster display from left to right in descending similarity fails to capture the full picture---two very similar images could be placed next to a third that is completely dissimilar, losing the multi-dimensional relationships in the data.
\end{enumerate}

To solve this, research has focused on techniques that preserve these complex similarity relationships while projecting the data into two or three dimensions. A major breakthrough in this area was t-SNE \cite{van2008visualizing}.

t-Distributed Stochastic Neighbor Embedding (t-SNE) \cite{van2008visualizing} is a nonlinear dimensionality reduction technique widely adopted for visualizing high-dimensional data. The method models pairwise similarities in both the original high-dimensional space and the low-dimensional embedding as probability distributions, seeking to minimize the divergence between them using a cost function based on Kullback-Leibler divergence:
\begin{equation}
	C = KL(P \,\|\, Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}},
\end{equation}
where $P$ represents similarities in the high-dimensional space and $Q$ those in the embedding. This approach excels at preserving local neighborhood structure. However, its high computational cost and limited scalability led to the development of alternatives.

Uniform Manifold Approximation and Projection (UMAP) retains the benefits of t-SNE while offering improved speed and better preservation of the data's global structure, making it the preferred choice for larger datasets.\\
Which is also why we utilized it in this work to reduce dimensionality of our image embeddings.


\section{Visualization of Text Content}
Visualizing text content is a complex problem with a multitude of approaches depending on several factors like content size, type and desired result/visualization.\\
Analyzing the most important topics of a document collection with a temporal aspect (showing which topics used to be relevant and which topics are relevant now, in a (over time)changing document collection such as presidential speeches) here one might utilize word clouds where word size and ordering play an important role see \cref{fig:wordClouds}  \cite{wordclouds}  different approach than visualizing a large corpus of scientific papers via an interactive interface with several components including a semantic link graph and a content summary section see \cref{fig:scientificPaperVisualization}\cite{visualizingPaperCollections}. Yet another approach for text documents and sentences is a word tree \cref{fig:wordTree} where words and unique sentence parts are displayed in a tree giving an overview of where and in what context certain words occur these however can get rather large for longer texts or documents \cite{wordTree}. Again others aim to visualize the core content of a social media text collection introducing other challenges and possibilities due to the data being mostly short and unstructured. This allows for a \enquote{variation} of a word tree that utilizes a more lose but still relevant ordering while filtering out more words deemed not relevant enough and displaying more important (usually more often occurring) words larger \cref{fig:sentenTree} \cite{sentenTree}.\\ 
Inspired by the last two approaches we present a variation of a word tree for prompts in form of a DAG with the goal of preserving sentence structure for easy similarity assessment while aggregating words and sentence sections as much as possible for a more manageable visual analysis.


  \begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{wordCloud.png}
	\caption{example of a word cloud about the US-presidential election 2008, we can see more relevant node being bigger, and that the words are loosely sorted by semantics, additionally some nodes are colored representing different statues of the nodes.   taken from \cite{wordclouds}}
	\label{fig:wordClouds}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{actionScienceExplorer.png}
	\caption{Interface of action scinece explorer showing the reference management 1-4, a citation statistic 5, the main visualization 6, a section for the citation context (7,9) and a summary section for multiple documents 8   taken from   \cite{visualizingPaperCollections}}
	\label{fig:scientificPaperVisualization}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{wordTree.png}
	\caption{Word tree showing occurrences and subsequent words of i have a dream in the famous speech of martin luther king taken from   \cite{wordTree}}
	\label{fig:wordTree}
	
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{sentenTree.png}
	\caption{example of a sentenTree about social media posts containing yelp and eat24, larger nodes are more relevant and some structure is give through edges taken from   \cite{sentenTree}}
	\label{fig:sentenTree}
\end{figure}

\chapter{Approach}

\section{Overview}

We present Prompt Vision, a visual-interactive system for exploring concept drift in text-to-image generation. Our approach combines automated prompt variation with dual interactive visualizations to help users systematically understand how prompt modifications affect generated outputs.

The system consists of three main components:
\begin{enumerate}
	\item \textbf{LLM-based prompt generation:} Automatically generates semantically related prompt variations based on user specifications
	\item \textbf{Dual visualization:} A scatter plot ordered on similarity and a DAG displaying prompt structure
	\item \textbf{Interactive exploration:} Linked views enabling users to trace relationships between prompts and images
\end{enumerate}

\ref{fig:system-overview} shows the overall workflow.

\section{Design Goals}

Based on the challenges identified in Section~\ref{sec:introduction}, we established the following design goals to guide our system design:

\textbf{1. Support Large-Scale Exploration}

\textit{Motivation:} Understanding concept drift requires exploring a large prompt-image space with varied concepts and content. Purely manual prompt crafting would be too slow and tedious to identifying systematic patterns.

\textit{Goal:} Enable generation and visualization of 400+ prompt-image pairs to facilitate comprehensive exploration of the prompt-image space.

\textit{Design impact:} This goal drove our decision to use automated LLM-based prompt generation rather than manual entry, and lead to the necessity of scale-able visualization techniques.

\textbf{2. Allow Dual-Exploration of Prompt-Image Space}

\textit{Motivation:} In order to understand prompt-image relations the user needs to be able to quickly identify what prompts belong to which images and vice versa.

\textit{Goal:} Create an easy to use and quickly to understand and most importantly linked visualization of prompt space and image space.

\textit{Design impact:} This goal led to the decision of using 2 separate graphs for prompt and image space, with methods of mapping to the other. 


\textbf{3. Balance Automation and User Control}

\textit{Motivation:} Fully automated systems remove user agency, while fully manual systems are labor-intensive. Users need to guide exploration while being able to automate labor intensive tasks to speed up exploration.

\textit{Goal:} Automate prompt variation generation while preserving user ability to manually add prompts.

\textit{Design impact:} This goal led to a hybrid approach where users specify what to vary (which words, how many alternatives) also enabling them to choose no modifications.

\textbf{4. Support Iterative Refinement}

\textit{Motivation:} When trying to understand concept drift, being able to iteratively add data points is crucial, this allows the user to discover and confirm patterns as well as trying out newly discovered concepts, which then can be viewed in the greater prompt-image space.

\textit{Goal:} Enable users to add new prompts based on observations and integrate them into existing visualizations.

\textit{Design impact:} This led to the decision to allow incremental prompt addition and dynamic visualization updates.

\section{Design Rationale}
\label{sec:design}
 \subsection{LLM-based Variation Generation} 
 We decided to use an LLM to generate the prompt alternatives, due to it being easy to use an integrate especially with JSON formated output. And it provides a crucial stochastic element when varying the prompts (variations aren't always the same due to stochastic nature of the model) while maintaining semantically fitting alternatives. Which is important for meaningful prompt-image space exploration.
\subsection{prompt engineering choices}
  In the prompt building we employed prompt engineering techniques namely in-context learning to improve the LLMs accuracy and performance and Structured output prompting. The later allows us to directly use the LLM response without further code to handle potential hallucination or formatting issues in post processing \cite{LLMoutputconstraints}, eliminating one avenue of errors or inaccuracy that might be introduced through that. 
\subsection{Dual Visualization}
We chose the dual visualization approach due to needing to visualize both prompt and image space together for a fast cognitively easy and large overview of the prompt-image space, which is why we chose a graph for each. Both graphs fit on one screen for an easy overview. 
\subsection{Grid}
We chose to extend the scatter plot with a grid due to the scatter plot images being somewhat small and partly overlapping. Which can extend to fully overlapping for large image collections. Making viewing the images without the grid almost impossible.
\subsection{Text-DAG}
For the prompt space we chose a text-DAG that is built in the following way: 
\begin{enumerate}
	\item \textbf{Clean the prompt} by removing all special characters, numbers, articles, pronouns, auxiliary verbs, and select prepositions (\textit{of, to, and, or, for, with, without, from, by, as}) while retaining words deemed to contribute to structure (\textit{in, at, on}) of generated image. We decided that due to the finding that function words and punctuation contribute minimally to model output from\cite{liu2022design}.
	
	\item \textbf{Form initial nodes} by iterating over each word in each prompt. We save position+word as unique node id, as well as the words layer, the word, it's direct relatives,whether the prompt ends after and all the other unique ids of the nodes of the other words in the current prompt (positions will correlate with the vertical layers but are changed for some node in case of collapse). We allow for only one node per word-position pair. For repeated word-position pairs we save their data to the corresponding node. 
	
	\item \textbf{Merge nodes} that have only one child and the node has no word-position pair that ended in their prompt, it is crucial to merge node data for scatter plot identification.
	
	\item \textbf{Collapse adjacent nodes} that contain identical entries if one of the nodes doesn't have a relative in the layer of the other, on collapse we need to make sure the node data is aggregated (we don't want to lose information this is especially important since we want to match all scatter plot elements to their corresponding nodes).
	
	\item \textbf{Apply multiple sweeps} of the barycenter heuristic which reduces edge crossings in graphs, resulting in a clearer graph layout.
\end{enumerate}
We chose this way to maintain as much of the prompt structure as possible while greatly aggregation similar prompts, making it easy for the user to identify the original prompts while presenting a spatially smaller cognitively easier to handle prompt representation. Additionally the DAG has a select and hover functionality: we display selected and hovered nodes red (for the same reason as in the scatter plot selection see \cref{sec:scatter}), when hovering over a node show all the nodes representing relatives of the selected and hovered node, this allows for reconstruction of the original prompt data from the DAG \cref{fig:dagPath}.  


\section{Embeddings and UMAP}
To meaningfully represent the images in a graph we needed a way to group or sort them which is a non trivial task as touched upon in \cref{sec:HighData}, we decided to sort the images based on similarity (the more similar the closer), as humans associate proximity with belonging together, creating a nice intuitive grouping. To achieve we needed a way to compare images, we decided to \enquote{vectorize} \cref{sec:vecImg} them for this we chose Img2Vec, as a state-of-the-art way to compute image embeddings (vectors), we also chose UMAP over t-SNE, to reduce the datas dimensions to 2D, as it better preserves local and global structures while being more compute efficient..
\section{scatter plot}
\label{sec:scatter}
We chose a 2D scatter plot to represent our dimensionally reduced embeddings as it is the graph most fitting to represent similarity based on distance, as well as aforementioned intuitively to understand due to the association of proximity and similarity. The graphs is 2D and corresponding as it is the most easy to view and grasp for our purposes as a 1D graph would be far to crowded due to having only 1 dimension to display data. While a 3D (and higher) graph would add complexity and effort when trying to effectively view the data.\\
Initially we only had circles representing the data but for quicker viewing we chose to represent the data points as circles displaying their corresponding image. We added color changing borders to them based on whether they are selected (red) or not (blue) allowing for an easy visual distinction between the two, while drawing attention to the selected ones as red is the highest-salience color (the color humans notice first/the most). To ease viewing of individual images we also introduced a zooming and panning feature
\section{Graph Interactions}
In order to understand prompt-image relations we needed a way for the user to map the prompts to the corresponding images and vice versa. To achieve this we we computed the unique ids (uids) as they are saved for each node in the DAG for the scatter plot (for each image we tokenize the prompt and save a set of word+positions to the image data), this now allows for mapping between the tow.\\
For the scatter plot we chose a size adjustable lens in combination with the aforementioned grid to select images, this allows for an easy and intuitive selection for the user. In the DAG we display all the nodes that are completely contained within the uids of at least of the selected images. This way we show all and only nodes corresponding to one the prompts of the images.\\ In contrast to map from the DAG to the scatter plot we chose a restrictive mapping: when hovering over a node in the DAG the uids of the displayed images need to contain all the uids of the hovered and selected nodes. This ensures that only images whose prompts contain all the words in the DAG selection are displayed.\\ This is an intuitive way to map the data as we show all the prompts responsible for the selected images as well as only the images related to (their prompts contain all the to the nodes corresponding words)the hovered nodes in the DAG. Henceforth the user is allowed and easy overview of corresponding prompt-image pairs. \cref{fig:scatterDagSelection,fig:dagScatterSelection,fig:graphInteraction}
\section{System Components}
\subsection{Prompting Help}
To assist the user in finding prompts quickly, the interface has a simple prompting tool, where the user can enter a prompt select which parts he'd like to vary (this is done via right-click to easy editing prompt as editing in a word would become impossible) as well as how many variations and how long they should be, then he can select whether he'd like to append or prepend something to the prompt as well whether he'd like to \enquote{mix} all the options (vary every variation option which each other instantly giving a large prompt image space). Then after the user selection and confirmation we build a prompt see \autoref{sec:prompts} with the given inputs where we  LLM called is GPT-5 the newest and most powerful model in the GPT series.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{promptInput2.png}
	\caption{prompt suggestion interface, with A: input area where one can rightclick to toggle the words to vary B: prompt suggestion customization options where one can select how many alternatives and how long they should be, C:history of last used prompts}
	\label{fig:promptInput}
\end{figure}



\subsection{Scatter Plot}
The scatter plot is implemented as a d3 svg, it displays the reduced image embeddings in a 2D space, in addition the the embedded data each data point has a path to their image and a set of uids. We display data points using small circular image thumbnails additionally the circles have blue colored borders when not selected and red ones when selected, to select a lens is used and a grid that displays the selected images sorted after similarity. When a DAG selection is made corresponding images are circled red while all other images are made less opaque. When hovering over a node the image aswell as the corresponding prompt displays in a popup window. 
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{scatterplotRawDots.png}
	\caption{Scatter Plot of 100 example images being plotted based on similarity }
	\label{fig:scatterDots}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{scatterplotRawHovered.png}
	\caption{ScatterPlot with node hovered displaying pop up windows with image and prompt}
	\label{fig:scatterHovered}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{scatterplotImages.png}
	\caption{ScatterPlot with images as data indicators we can see the dots have been replace with blue outlined image circles  }
	\label{fig:scatterImages}
\end{figure}


\subsubsection{Image Grid}
\label{sec: grid}
The grid displays images selected in the scatter plot, to provide a quick and comprehensive overview of the selected images we needed to sort the images based on similarity.
At first we wanted to use a pixel based comparison methods \cref{sec:pixComp} such as SSIM, however SSIM or similar methods are computationally somewhat expensive to service a live application given the current limitations of science and technology.  Therefore displaying the grid with a pixel based similarity measure only works live if similarity values are pre-computed.
Due to these limitations we looked for another method, realizing that we already had a geometric ordering of the image data thanks to UMAP we reused the already existing values, by ordering the data in the lens selection geometrically, displaying images hierarchical from top-to-bottom-left-to-right \cref{fig:scatterGeometric}, achieving arguably better results with response times suitable for a live application.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{gridNoSSIM.png}
	\caption{ScatterPlot with Lens selection displaying selected images in grid unsorted}
	\label{fig:scatterNoSSIM}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{gridSSIM.png}
	\caption{ScatterPlot with Lens selection displaying selected images in grid sorted with SSIM, grid displays arguably a coherent sorting with some images appearing out of space}
	\label{fig:scatterSSIM}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{gridGeometric.png}
	\caption{ScatterPlot with Lens selection displaying selected images in grid with geometric sorting, the grid displays coherent image similarity from top to bottom}
	\label{fig:scatterGeometric}
\end{figure}
\subsection{Directed Acyclic Graph}
In the following the DAG progression for different steps will be shown to visualize the changes. In order for them to be easier to follow only 5 prompts have been chosen. We see that initially relations between the prompts are hard to glean, even with only 5 prompts comparing the text and identifying the similarities and difference is quite a chore for the human visual complex. Removing stop and function words makes it a little easier due to the text content decreasing however it's still hard and will only exponentially increase in difficulty when adding more prompts (imagine trying to compare 50 or 100 prompts "by hand").\\
Just by aggregating words at the same position and indicating their relation via edges we can already see noticeable improvements, these are further pronounced when collapsing nodes containing "phrase words", words that appear at their position only next to one relative.\\
Overall the DAG displays words following a horizontal order. It is meant to be read left to right if nodes have a link that means at least one prompt in the original data contained that word pairing in that order. The difference vertical layers correspond to the position of the word in the original prompt. A user is able to select and hover over nodes (which will display red)\cref{fig:dagPath} in the DAG, when hovering over a node the DAG displays all nodes that are relatives of the selected and hovered node, here relatives are independent of the graph structure they instead refer to relatives in the original prompt data (words that occurred in the same prompt). This selection will also trigger the display of all nodes in the DAG that correspond to prompts containing all the words in the selection.
\begin{figure}[H]        % 'h' = place here (optional)
	\centering           % centers the image
	\includegraphics[width=\linewidth]{DAGNothingApplied2.png}
	\caption{Raw prompts with no aggregation or processing}
	\label{fig:dagPure}  % optional: for referencing
\end{figure}
\begin{figure}[H]        % 'h' = place here (optional)
	\centering           % centers the image
	\includegraphics[width=\linewidth]{DAGTokenized.png}
	\caption{Prompts with stop and some function words removed}
	\label{fig:dagTokenized}  % optional: for referencing
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth, trim=4cm 1cm 4cm 1cm, clip]{DAGNotCollapsed.png}
	\caption{DAG with nodes at same position merged with edges indicating relationships noticeably reduced size }
	\label{fig:dagMerged}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth, trim=4cm 1cm 4cm 1cm, clip]{DAGCollapsed.png}
	\caption{DAG with nodes collapsed if there are unique to that order and sequence, aswell as neighbourhing nodes collapsed if they have the same content (not the case here so no changes from second collapse)}
	\label{fig:dagCollapsed}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth, trim=4cm 1cm 4cm 1cm, clip]{DAGPathSelected.png}
	\caption{Node selected showing the path (ancestors,descendants) of the selected nodes relative to the original prompt input data}
	\label{fig:dagPath}
\end{figure}





















\begin{figure}[H]        % 'h' = place here (optional)
	\centering           % centers the image
	\includegraphics[width=\linewidth]{generalOverview.png}
	\caption{Figure showing a general overview of the system components and their procedural order, beginning with the  prompt input interface, followed by the LLM generating prompt variations then the Image generation models next the embeddings are computed for the images through Img2Vec and UMAP. Then the embeddings with the images  and prompts are sent to the visualization layer, where the scatter plot displays the images positioned based on their embeddings and the DAG the aggregated prompt data}
	\label{fig:system-overview}  % optional: for referencing
\end{figure}


\begin{figure}[H]        % 'h' = place here (optional)
	\centering           % centers the image
	\includegraphics[width=\linewidth]{overview.png}
	\caption{Figure showing the general workflow of the framework, the user is able to interact with the DAG and scatter plot, they can also utilize the image generation pipeline where they first enter their initial prompt into the Prompt Input field and then select how they want to modify the prompt, this is then passed onto the backend which automatically generates a prompt for the LLM, the LLM outputs new prompts based on the user specifications which are then generated and finally passed onto the DAG and scatter plot for display}
	\label{fig:workflow-overview}  % optional: for referencing
\end{figure}












\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{graphInteraction1.png}
	\caption{graph showcasing the general interactions between DAG and scatter plot, we can see that a scatter plot selection is sent to the EventBus (a d3 element), who then triggers the grid display and the mapping to the corresponding DAG elements, this works vice versa for a DAG selection expect the grid is not involved}
	\label{fig:graphInteraction}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{scatterDagSelection.png}
	\caption{figure showing a lens selection in the scatter plot with matching data in DAG, showing us that we only selected images corresponding to prompts containing the word tiger and but not apple}
	\label{fig:scatterDagSelection}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{dagScatterSelection.png}
	\caption{figure showing selection of the word sitting in the DAG and the matching data in the scatter plot}
	\label{fig:dagScatterSelection}
\end{figure}
\section{System Workflow}
The workflow begins with the user entering a prompt in the Prompting Help text-field they then selects the words they want to modify by right-clicking them. Next he selects how many variations they'd like and how long they should be, as well as if they want to prepend or append something to the prompt and if yes the length. Lastly they select whether they'd like only as many prompts as they selected variations or the Cartesian product of all variations (mix all variations with each other). They then wait for the creating of the images and subsequent display in DAG and scatter plot. They then explore the prompt-image relationships in DAG and scatter plot. Finally they can iteratively repeat the whole process creating a larger more informative prompt-image space. \cref{fig:system-overview}
\section{Implementation Details}
To realize the chosen design a web application utilizing d3 was implemented
\subsection{d3}
D3.js (Data-Driven Documents) is a JavaScript library for creating interactive and dynamic data visualizations in web environments. Unlike traditional libraries that provide fixed templates and charts, D3 gives developers fine-grained control over how data is bound to the graphical elements. It utilizes web standards such as HTML, SVG, and CSS, enabling flexible and highly customizable visualizations.

The key strength of D3 is its declarative data binding: datasets can directly drive the creation, transformation, and styling of visual elements. This makes it possible to build anything from simple bar charts to complex, interactive dashboards or exploratory visual analytics systems. Because of this versatility, D3 has become one of the most commonly used tools for web-based data visualization in both research and industry.

\subsection{General Framework}
For the general framework of the web-application html, Java Script and Cascading Style Sheets was utilized for the frontend and python for the backend.
This allows for fast light weight prototyping and easy adaptability. For the generative tasks we used as api call to GPT-5 a LLM and for the image generation task we used a stable diffusion model \cite{stableDiffusion}.
                                                                        

\chapter{Use Case example}
A user wants to create an image using generative AI for a presentation. However, they are frustrated, during their last generation session, they had to adjust their prompt dozens of times before achieving a satisfactory result. Seeking a more efficient approach, they turn to prompt vision to gain better insight into how prompt modifications affect generated outputs, hoping to reduce time and iterations in future image creation tasks.\\
First the user selects a model \footnote{Model selection is not implemented in the prototype} and enters a prompt in the dedicated field. They really like animals and come up with a scenario for an elephant as well as a quality tag they remember from last time \enquote{A photo of an elephant munching on a stack of hay in a barn, extremely high quality}. They select which words to vary and specify the desired length for alternatives \cref{fig:usecasePrompt1}. Next they choose to let the LLM mix all alternatives with each other curious to explore the different variations. While waiting on the tool response they decide to take a coffee break. 

Upon returning, they are initially surprised by the number of images created, then begin interacting with the DAG visualization by hovering and selecting nodes, aiming to understand how changes in the prompt have affected the images \cref{fig:usecaseQualityDag}. 

After selecting and observing different word combinations, as well as zooming and panning in the scatter plot to better identify images, they note that the differences when using image or photo in the prompt seem minimal, with only drawing looking less real. Next they discover that against their expectations the difference between using fantasy or realistic is quite small. Using cute however drastically changes the output most of the time, making the animals appear more cartoonish with child-like features. Then they observe that the model seems to generate simple poses like sitting well, but fails to show more complex poses, like spying, in the majority of images, and completely fails to display munching. They also notice that the backgrounds are always accurate and the model performs way better when tasked with displaying the apple and the hay stack than the fish. \\
Then the user notes an anomaly in the displayed data: a series of black-and-white images. Thus they use the lens to investigate what part of the prompt could have caused that artifact \cref{fig:usecaseQualityScatter}. Looking at the highlighted words in the DAG they notice that there are multiple options for everything except for drawing, elephant, and barn making the last 3 the likely culprits. They then hover over each option, noticing that 'drawing' has by far the largest portion of black-and-white images, identifying it as the cause.\\
Wanting to experiment with another style they repeat the process with a slightly varied prompt: \enquote{A photo of an elephant munching on a stack of hay in a barn, oil painting} adding the newly generate images to the pre-existing. They compare the new and old images by selecting different word combinations in the DAG. The oil painting style mostly produces more saturated colors and softer edges. However, when examining images tagged with both \emph{realistic} and \emph{oil painting}, they observe that the outputs have more painting qualities while losing photo realism a result of the conflicting descriptors. After exploring further they note that the other previous findings hold true for the newly added images for example fish still generating rarely while the background is always accurate.

 With these newfound insights about how different prompt components affect the generated outputs, they close the tool and proceed to create their presentation image, now better equipped to craft an effective prompt.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{usecasePrompt1.png}
	\caption{Figure showing the prompt field where the user types the prompt and customizes the suggestion opting for 2 alternatives for each selected word each being 1 word long}
	\label{fig:usecasePrompt1}
\end{figure}
\begin{figure}[H]
		\centering
	\includegraphics[width=\linewidth]{usecaseQualityDag.png}
	\caption{Figure showing an example selection in the DAG, cow and realistic have been selected while image is being hovered, the corresponding images in the scatter plot show cows while non selected images are made less opaque, selected images are highlighted red}
	\label{fig:usecaseQualityDag}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{usecaseQualityScatter.png}
	\caption{Figure showing a lens selection of black and white elephant images in the scatter plot, the grid is displaying the corresponding images in large without overlay and the DAG is showing the corresponding prompt paths}
	\label{fig:usecaseQualityScatter}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{usecaseQualityPaintingDagZoom.png}
	\caption{Figure showing the selection of \enquote{oil painting} in the DAG while being zoomed in on the images, we see that most images have less defined lines more saturated colors and painting like qualities }
	\label{fig:usecaseQualityPaintingDagZoom}
\end{figure}



\chapter{Discussion}
\label{chap:zusfas}

Our approach, which visualizes the prompt-image space through two linked interactive graphs, enables users to efficiently explore the relationships between text prompts and their corresponding generated images, thus gaining an overview of the models concept drift. By interacting with both the scatter plot and the directed text-DAG, and iteratively as well as adding new prompt-image pairs based on their observations, users can quickly identify how specific prompt modifications influence the visual output. This process facilitates the generalization of the model's underlying behavior, allowing users to deduce its common tendencies such as a struggle with complex poses (e.g., "munching"), a tendency to generate in multiple styles given certain modifiers (e.g. black-and-white as well as color images for "drawing"), and a consistent accuracy in rendering backgrounds. These insights have practical value for prompt engineering especially for novice users. Understanding which prompt components reliably affect outputs versus which produce inconsistent results allows users to craft more effective prompts with fewer iterations. Rather than trial-and-error experimentation, users can develop systematic strategies based on observed patterns.

The effectiveness of this method depends on sufficiently similar data. If the prompt-image pairs are too dissimilar, the DAG would fail to aggregate nodes meaningfully, resulting in excessive branching and an unwieldy structure. Similarly, the scatterplot would devolve into a disorganized set of points lacking meaningful clusters, thereby hindering the discovery of patterns. This is not a concern when the user uses the LLM-prompt-suggestion feature can however become one when adding many new dissimilar prompts.

A technical limitation of our tool is its scalability. For large datasets exceeding 1,000 images, two significant challenges arise: long waits for responses from the image generation model and significant latency for the D3.js zooming and panning interactions. Furthermore, a crowded scatter plot necessitates the use of the lens and grid selection tools to resolve overlapping images, which adds a layer of interaction complexity.

Our work occupies a distinct position in the landscape of prompt visualization tools. While systems like PromptMagician \cite{PromptMagician} provide granular control over model hyper parameters and attention mechanisms with a focus on clustered representative images, our tool prioritizes breadth of exploration over depth of control. 
This trade-off has the disadvantage of: Lacking the more precise fine tune able control over attention weights and hyper parameters tools like PromptMagician provide. However it has a lower cognitive load that makes the tool accessible for users exploring unfamiliar models or beginning prompt engineering. As well as providing a comprehensive view of the prompt-image space that might reveal patterns that could be missed when focusing on a few representative examples.




















\chapter{Conclusion}




We presented Prompt Vision, a visual-interactive system for exploring concept drift in text-to-image generative models. Our dual visualization approach—combining a scatter plot showing semantic similarity with a DAG showing prompt structure—enables users to efficiently identify how prompt modifications trigger conceptual shifts in generated outputs.

Our key contributions include: (1) a scalable visualization approach supporting exploration of 100s of prompt variations, (2) LLM-based prompt variations maintaining semantic relevance, (3) design patterns for our prompt engineering tool.

Through our use case, we demonstrated that users can identify systematic model behaviors such as reliable background rendering but struggles with complex behaviors such as spying. This can infer more effective prompt engineering strategies. Providing practical value for novice users or when exploring new models. We enable more systematic exploration over pure trial-and-error experimentation.

Our approach has limitations: it requires sufficiently structural similar text data and sufficiently visually similar image data (dissimilar prompts fragment the visualizations, while dissimilar images hinder visual analysis). It also faces scalability challenges beyond ~1,000 images (depending on hardware) as D3 struggles with the zoom and pan behavior. We only utilized one model to generate the test data and currently don't support live-generation in the prototype. These are challenges future work could address. Additionally trying to incorporate model attention and hyper parameters in the visual approach could provide the user with additional insights. 

While systems like PromptMagician and promptCharm \cite{promptCharm,PromptMagician} provide fine-grained parameter control, and Promtify and promptThis \cite{brade2023promptifytexttoimagegenerationinteractive,guo2024prompthisvisualizingprocessinfluence} focus on smaller scale  visualization, we prioritize breadth of exploration, providing a holistic overview of concept drift. 



\printbibliography

All links were last followed in October 2025.

\appendix
%\input{latexhints-english}
\chapter{appendix}
\section{prompts}
\label{sec:prompts}
This section shows how the code for the prompt creation in python based on user input.
Prompt for normal varying:
\begin{verbatim}
	if prepend and append:
	extra_instruction = (
	f"Prepend and append {lengthPrependAppend} words to each prompt. "
	f"These can range from setting to style to image clarity and consistency improvements. "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	elif prepend:
	extra_instruction = (
	f"Prepend {lengthPrependAppend} words to each prompt. "
	f"These can range from setting to style to image clarity and consistency improvements. "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	elif append:
	extra_instruction = (
	f"Append {lengthPrependAppend} words to each prompt. "
	f"These can range from setting to style to image clarity and consistency improvements. "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	
	
	
	examples = """
	Example 1:
	Input prompt: "a cat sitting on a windowsill"
	Words to vary: "cat, sitting"
	Number of alternatives: 2
	Prepend 3 words
	
	Output:
	{
		"prompts": [
		"a cat sitting on a windowsill",
		"Golden hour lighting a tabby resting on a windowsill",
		"Photorealistic digital art a kitten perched on a windowsill"
		]
	}
	
	Example 2:
	Input prompt: "futuristic cityscape"
	Words to vary: "futuristic"
	Number of alternatives: 2
	Append 4 words
	
	Output:
	{
		"prompts": [
		"futuristic cityscape",
		"cyberpunk cityscape, neon lights, rain-soaked streets",
		"sci-fi cityscape, highly detailed, octane render"
		]
	}
	
	Example 3:
	Input prompt: "mountain landscape at sunset"
	Words to vary: "mountain, sunset"
	Number of alternatives: 2
	Prepend 2 and append 2 words
	
	Output:
	{
		"prompts": [
		"mountain landscape at sunset",
		"Oil painting alpine peaks at dusk, warm colors",
		"Dramatic composition rocky cliffs at twilight, cinematic lighting"
		]
	}
	"""
	
	user_prompt = (
	f"{examples}\n\n"
	f"Now apply the same approach:\n"
	f"Input prompt: {prompt}\n"
	f"Words to vary: {wordsToChange}\n"
	f"Number of alternatives: {numAlternatives}\n"
	f"Alternative length: {alternativeLengthWhenWordSelected}\n"
	f"{extra_instruction}\n\n"
	f"Generate the original prompt plus {numAlternatives} varied alternatives following the pattern shown above and return the output in a Json format where each promt is a new entry."
	)
\end{verbatim}
Prompt for mixing all variations: 
\begin{verbatim}
	Inhalextra_instruction = ""
	if prepend and append:
	extra_instruction = (
	f"Prepend and append {lengthPrependAppend} words to each prompt. Treat these the same as 'words to vary'"
	f"These can range from setting to style to image clarity and consistency improvements.  "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	elif prepend:
	extra_instruction = (
	f"Prepend {lengthPrependAppend} words to each prompt. Treat these the same as 'words to vary' "
	f"These can range from setting to style to image clarity and consistency improvements. "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	elif append:
	extra_instruction = (
	f"Append {lengthPrependAppend} words to each prompt. Treat these the same as 'words to vary' "
	f"These can range from setting to style to image clarity and consistency improvements. "
	f"Return your answer as JSON where each prompt is a new entry."
	)
	
	examples = """
	Example 1:
	Input prompt: "a grey cat sitting on the floor"
	Words to vary: ["grey", "cat"]
	Alternatives per word: 3
	
	Step 1 - Generate alternatives:
	- "grey" → ["orange", "black", "brown"]
	- "cat" → ["kitten", "dog", "persian"]
	
	Step 2 - Generate all combinations:
	{
		"word_variations": {
			"grey": ["orange", "black", "brown"],
			"cat": ["kitten", "dog", "persian"]
		},
		"combined_prompts": [
		"a orange kitten sitting on the floor",
		"a orange dog sitting on the floor",
		"a orange persian sitting on the floor",
		"a black kitten sitting on the floor",
		"a black dog sitting on the floor",
		"a black persian sitting on the floor",
		"a brown kitten sitting on the floor",
		"a brown dog sitting on the floor",
		"a brown persian sitting on the floor"
		]
	}
	
	Example 2:
	Input prompt: "sunset over mountains"
	Words to vary: ["sunset", "mountains"]
	Alternatives per word: 2
	
	Step 1 - Generate alternatives:
	- "sunset" → ["dawn", "twilight"]
	- "mountains" → ["hills", "peaks"]
	
	Step 2 - Generate all combinations:
	{
		"word_variations": {
			"sunset": ["dawn", "twilight"],
			"mountains": ["hills", "peaks"]
		},
		"combined_prompts": [
		"dawn over hills",
		"dawn over peaks",
		"twilight over hills",
		"twilight over peaks"
		]
	}
	
	Example 3:
	Input prompt: "a red sports car on the highway"
	Words to vary: ["red", "sports car"]
	Alternatives per word: 2
	
	Step 1 - Generate alternatives:
	- "red" → ["blue", "silver"]
	- "sports car" → ["sedan", "SUV"]
	
	Step 2 - Generate all combinations:
	{
		"word_variations": {
			"red": ["blue", "silver"],
			"sports car": ["sedan", "SUV"]
		},
		"combined_prompts": [
		"a blue sedan on the highway",
		"a blue SUV on the highway",
		"a silver sedan on the highway",
		"a silver SUV on the highway"
		]
	}
	"""t...
\end{verbatim}


\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Versicherung
\end{document}
