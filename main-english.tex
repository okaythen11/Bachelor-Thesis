% !TeX document-id = {4719ec8f-70f7-4005-9222-7d90db5fa2e2}
% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-


% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iffalse
\let\ifenglisch\iftrue
\input{pre-documentclass}
\documentclass[
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-sided printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}


\usepackage[
  title={},
  author={Oliver Seiz},
  type=bachelor,
  institute=visus, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Informatik},
  examiner={Dr. Steffen Koch},
  supervisor={Jena Satkunaranjan},
  startdate={April 10, 2025},
  enddate={Oktober 10, 2025}
]{scientific-thesis-cover}

\input{acronyms}

\makeindex

\begin{document}

%tex4ht-Konvertierung verschönern
\iftex4ht
  % tell tex4ht to create pictures also for formulas starting with '$'
  % WARNING: a tex4ht run now takes forever!
  \Configure{$}{\PicMath}{\EndPicMath}{}
  %$ % <- syntax highlighting fix for emacs
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
\pagenumbering{arabic}
\Titelblatt

%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftriplepagestyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftriplepagestyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}



%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis
\section*{Abstract}

<Short summary of the thesis>

\cleardoublepage

%Solely for German courses of study
\section*{Kurzfassung}

<Kurzfassung der Arbeit>

\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

% Bei einem ungünstigen Seitenumbruch im Inhaltsverzeichnis, kann dieser mit
% \addtocontents{toc}{\protect\newpage}
% an der passenden Stelle im Fließtext erzwungen werden.

\listoffigures
\listoftables

%Wird nur bei Verwendung von der lstlisting-Umgebung mit dem "caption"-Parameter benoetigt
%\lstlistoflistings
%ansonsten:
\ifdeutsch
  \listof{Listing}{Verzeichnis der Listings}
\else
  \listof{Listing}{List of Listings}
\fi

%mittels \newfloat wurde die Algorithmus-Gleitumgebung definiert.
%Mit folgendem Befehl werden alle floats dieses Typs ausgegeben
\ifdeutsch
  \listof{Algorithmus}{Verzeichnis der Algorithmen}
\else
  \listof{Algorithmus}{List of Algorithms}
\fi
%\listofalgorithms %Ist nur für Algorithmen, die mittels \begin{algorithm} umschlossen werden, nötig

% Abkürzungsverzeichnis
\printnoidxglossaries

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Introduction}

This thesis starts with \cref{chap:k2}.

We can also typeset \verb|<text>verbatim text</text>|.
Backticks are also rendered correctly: \verb|`words in backticks`|.

With the growing capabilities of AI-models in recent years, its usage has become more widely adopted, with some sources claiming 280 million global users in 2024 \cite{AI-users} and projections of up to 1 billion users by 2030.

Perhaps the most popular models LLMs capable of impressive language understanding and reasoning tasks, besides them there are generative models which have found a wide variety of use cases for music, video, and mainly image generation. 

Perhaps the most popular AI models today are LLMs, which demonstrate impressive language understanding and reasoning capabilities. Alongside them, generative models have emerged, finding use cases in music, video, and mainly image generation. These models can create entire complex, high-quality scenes within seconds, thanks to major advances in recent years. Leading examples such as MidJourney \cite{midjourney}, Stable Diffusion \cite{stable-diffusion}, and GPT-4o \cite{GPT-4o} are able to produce photo-realistic images as well as artworks in many different styles and forms. They allow people to explore their creativity with few technical limitations, shifting the main challenge toward effective goal driven usage of them. 

The most common approach when using image generation models is providing a textual prompt from which the model then generates an image. Other forms of prompts include pictures or sounds, but text remains the most widely used. The problem with the textual  approach is its ambiguity, the user does not know how exactly the model will process the prompt or what the main drivers for changes in image output are. More concisely to the average user, the model behaves like a black box. Consequently getting the desired result by tweaking the prompt can be challenging and sometimes feel arbitrary.

The goal of this thesis is therefore to conceptualize and develop a visual-interactive approach that helps to understand how changes in a prompt impact the generated image.For this we will introduce an interactive framework between 2 graphs as well as a prompting aid for the user.
\chapter{Background}
\label{chap:k2}
This section serves as an introduction to topics, concepts and techniques that are relevant to understanding this thesis, introducing the topics of generative models, diffusion models Large-Language-Models (LLMs), prompt engineering, vectorization of images, vector dimensionality reduction, pixel based comparison and visualization techniques. 
\section{Generative models}
%strutcure
%Definition: Learning data distributions to generate novel samples
%Key distinction from discriminative models

%Broad applications: image synthesis, text generation, audio, video

Generative models are a relatively recent phenomenon in the field of AI, they aim to create new unseen data that fits the data they have been trained upon. The most prominent of them are image generation models with the first models capable of generating complex images emerging in 2013 they are called VAE (variational autoencoders) named after the architecture they are based on (Auto-Encoding Variational Bayes) and work by making stochastic sampling differentiable \cite{kingma2022autoencodingvariationalbayes}, however early adaptations resulted in blury and low resolution images.
Then in 2014 goodfellow et. all {goodfellow2014generativeadversarialnetworks} proposed a new process for estimating generative models using an adverserial approach, this is realized by using a two part architecture in training, a generator and a discriminator both neural networks. The generator outputs a sample from noise and the discriminator tries to classify whether that sample is a from a real data distribution or generated based on the result of the discriminator the generators weights are adjusted to result in a less distinguishable output. This approach results in more realistic higher quality images, since the generator is trained to "fool" the discriminator into thinking its output is real. 

In 2017 Vaswani et al pusblished their breakthrough paper \verb|“Attention Is All You Need”| \cite{vaswani2023attentionneed} introducing transformer architecture for AI %self-attention, positional encoding, encoder–decoder structure 
originally intended for sequence-to-sequence tasks their adaptation too generative AI was very successful, with early models using transformer architecture like GPT-1 \cite{GPT-1} and BERT \cite{devlin2019bertpretrainingdeepbidirectional} being quite capable for their respective tasks. However early transformer models for image generation such as Image Transformer \cite{parmar2018imagetransformer} struggeled with producing high quality images, while later attempts and hybrid architecture led to better performance generating images. Then in 2020 Ho et. al published their landmark paper "Denoising Diffusion Probabilistic Models" where they adpated diffusion models for image generation running on transformers.
Subsequent improvements of the approach make diffsion models the state-of-the-art for image generation, which is why they were used for image generation in this work. %add reference to stable diffusion or models being used for live generation

\section{LLMs}
Large-Language-Models (LLMs) have emerged in the field of natural language processing (nlp) but have since transcended their original purpose, most of them function by predicting the next token utilizing a transformer architecture and large scale pre-training on massive amounts of data they are able to achieve impressive results text generation and summarization to translation, reasoning, and dialogue. Their big breakthrough was in 2020 with Brown et al. publishing GPT-3 \cite{brown2020languagemodelsfewshotlearners} first showing  that scaling the number of parameters and training data substantially improves performance. However despite their impressive capabilities LLMs are faced with several drawbacks, they are computationally expensive requiring large resource hungry data centers to train and service request at scale, additionally they may show bias or inaccuracy in their responses depending on the underlying training data, which makes sourcing quality data at the required scale a major challenge LLM developers face. Another negative is their non applicability in safety critical environments due to hallucination, where the model generates plausible but incorrect outputs.

\section{prompt engineering}
In order to use the LLMs introduced in the last section we need to "prompt" them that means entering a text or an image with instructions for the LLM. However what and how we type the prompt is important for the performance of the model as zhang et al. \cite{zhang2025understandingrelationshippromptsresponse} and li et al. \cite{li2024effectsdifferentpromptsquality} have shown for their respective selected tasks. This importance of the prompt has created a whole new field of research called "prompt engineering" where people are trying to find techniques and tricks to improve the LLM output or even get past certain restrictions. 
To achieve accurate results some prompt engineering principles were also employed in this thesis.

\section{vectorization of images}
Comparing images is an old and non-trivial problem that is also afflicted by subjectivity, as humans might rate different images as more similar or dissimilar based on individual factors. Human evaluation also takes longer and is more costly which is why a mathematical method is crucial. 
One way is to transform an image into something with many pre-existing comparison methods such as a vector where mathematical similarity measurements are well established. There are several ways to turn an image into a vector such as the "classical" approach where feature extraction is hand-engineered such as SIFT \cite{inbook}, however due to greater generalizability, automatic feature extraction and usually better performance methods using specifically trained machine learning models are generally preferred. Which was also the case in this thesis, where the Img2Vec library \cite{patel2019img2vec} was utilized.

\section{vector dimensonality reduction}
In dimensionality reduction we try to reduce the dimension of a given data points while trying to preserve certain properties and relationships between the data points, in our case we are interested in the similarity of the given data points. I utilized Umap which builds a nearest neighbour graph utilizing fuzzy sets in the higher dimensional space and then tries to preserve this realtionship in lower dimensional spaces usally 2D or 3D. 
As data pojnts we utilize the vectors we get from the previous section where we try to preserver their similarity while projecting them onto 2d space thus we are able to plot them on a simple 2d graph which is visually one of the most intuitive for humans to grasp. 

\section{pixel based comparison}
Evaluating the similarity between images is an important step in many image processing tasks such as compression, denoising, and enhancement. Traditional measures like Mean Squared Error (MSE) or Peak Signal-to-Noise Ratio (PSNR) compare images directly at the pixel level. While simple and efficient, these methods often do not reflect how humans perceive visual quality. To address this gap, more perceptually similar approaches such as the Structural Similarity Index (SSIM) \cite{ssim} have been introduced. SSIM goes beyond raw pixel differences by incorporating luminance, contrast, and structural information, which makes it better aligned with human visual perception. As a result, SSIM has become a widely used standard for assessing image quality in both research and applied settings.



LaTeX hints are provided in \cref{chap:latexhints}.

%\blinddocument

\chapter{Related Work}
This chapter gives an overview of pre-existing concepts and work related to this thesis 


\section{Prompt Engineering and Visualization}
Mishra et al. (2023) \cite{mishra2025promptaidpromptexplorationperturbation} introduced PromptAid, an interactive system for prompt exploration and iteration when using large language models. Their approach aims to support inexperienced users in creating more effective prompts. Their framework contains three semi-automated strategies: keyword perturbations (replacing or varying keywords), paraphrasing perturbations (rewriting or changing phrasing), and "selecting an optimal set of in-context few-shot examples" which they aim to visualize for the user. This allows users to easily generate prompt iterations analyze and compare the outcomes and iteratively adjust the prompt. \\

Similarly, Brade et al. (2023) \cite{brade2023promptifytexttoimagegenerationinteractive} introduced PromptiFy a framework for prompt suggestion and result visulaziation, they achieve this by utilizing three interrative main methods in their framework 1."automatic prompt extension and suggestion" here the user can first select a general theme for which the framework will query an LLM for suggestions and then an art style, then 2."image layout and clustering by similarity" where the suggested prompts will be send to the underlying model for generation and then displayed in a graph based on their similiarity (the closer the more similar), lastly
 3."automatic prompt refinement suggestions." where an LLM suggest refinements to the prompt to achieve different outcomes. 
 This process again supports the user in finding suitable prompts.\\



 Guo et al. (2024) \cite{guo2024prompthisvisualizingprocessinfluence} presented PrompTHis, a visual analytics tool that highlights the process and influence of prompt editing during text-to-image creation. They achieved this by remebering the history of prompt and image pairs and plotting them in a specially designed graph they call an Image Variant Graph. This supports the user in aquiring an understanding of the impact of prompt changes to the image, enabeling them to closer control and influence the generation of the model. 



add figures for promptaid and promptthis?
Describe relevant scientific literature related to your work.
\section{Visualization in Generative Models}
\section{Visualization of large data sets and high dimensional data}
In order to visualize large data sets graphs usually are the go to approach however some things are harder to plot than others, while it might be easy to plot a quarterly profits report plotting high dimensional data like text, videos or images is a different challenge especially when wanting to plot them relative to each other. In our case plotting images based on their similarity, this is challenging due to a lot of traditional measures to determine the similarity of images returning a single number (depending on the implementation usually the closer that number to 0 the similar the images), additionally the similarity is usually measured between 2 images then for larger data sets with possibly hundreds of images we'd have to chose 1 starting image from the data set and compare it all the others to gain our similarity score. This has some potential issues 1. it might not give an accurate representation of the images towards each other when plotted (2 very different images could end up with a very similar difference score to the chosen starting image) making such a measure potentially ineffective for visualizing data. 2. This makes plotting the data in a meaningful way hard as viewing the data on a 1D number line doesn't utilize the full screen space and observes have a harder time to view and relate all the data (as they have to pan from left to right having a larger distance between data points). Also plotting the data in a raster where we go from left to right in descending similarity wouldn't properly quantifying how similar images are  (2 images that are very similar to each other could be next to an image very dissimilar from these first 2), losing that entire dimension of the data in the visualization. \\
Thus research has been done to be able to preserve the similarity relationship while utilizing 2 or 3 dimensions with the first major breakthrough in that direction being t-SNE \cite{van2008visualizing}.

t-Distributed Stochastic Neighbor Embedding \cite{van2008visualizing} is a
nonlinear dimensionality reduction technique that has been widely adopted for
visualizing high-dimensional data. The method models pairwise similarities between
points in both the original space and the low-dimensional embedding as probability
distributions, and seeks to minimize their divergence. The cost function is given by
\begin{equation}
	C = KL(P \,\|\, Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}},
\end{equation}
where $P$ represents similarities in the high-dimensional space and $Q$ those in
the embedding. This approach effectively preserves local neighborhood structure.
However due to its high 
computational cost and limited scalability.
Uniform Manifold Approximation and Projection (UMAP) has been proposed as an
alternative that retains the benefits of t-SNE while offering improved speed and
better preservation of global structure. Making it the prefered choice for larger data sets.


\section{visualization of sentences}


\section{Concept Drift in Generative models}
Traditionally, concept drift in machine learning refers to a change in the mapping between input data and output labels over time. A model is trained on a particular data distribution, but as the relationship between data features and model classification shifts, its predictions become less reliable. For example, a spam email detector trained on data from five years ago may struggle today because spammers use new patterns and tactics that the model never learned, causing it to missclassify more modern spam.\\
In this thesis concept drift refers to the changing the intent of the user while prompting the image generation model. Where in a normal use case it is likely that the prompt will be adjusted altered modified and specified many times in order to achieve the desired end result. This work aims to visualize that process and easy narrowing in on a desired prompt by providing an overview between the image and the prompt space hopefully allowing the user to identify what parts of the image changes based on the changes in the prompt.



where a model has been trained under a certain data set (the training data), but the data it is now applied on has shifted from the training data set, making the model less effective 
\chapter{Approach}
This work aims to highlight the relationship between prompt and generated image in order for the user to better understand how changes in the prompt affect the generated images. \\
A major challenge is visualizing the connection between the prompt and the image especially for many the prompt image pairs that are normally created during the creative process when utilizing an image generation model.\\
For this an interactive approach is chosen  between a visualization of the images and the prompts.\\ Research led to chose a 2D scatterplot for images where circles with the indivudial image inside are vectorized and  displayed based on their similarity determined by U-map. To display the prompts and allow their easy readability and association with its corresponding prompt a sideways dag-tree was chosen where nodes are computed via the following steps 1. clean up the prompt, removing all special characters numbers and remove articles, pronouns, Auxiliary verbs and some prepositions (of, to, and, or, for) aswell as (with, without, from, by, as) while keeping others (in, at, on) that are deemed to be semantically significant for the image generation. 2. The inital nodes are formed consisting of the word and the position at which it occurs we only look at unique word positions words that appeared already are disregarded though their relationships (parent-child) are recorded, additionally we save whether the word has a child or whether any prompt ends with the word position pair. 3. We merge nodes that only have 1 child and which no prompt ends with. 4. collapse adjacent nodes with same entries 5. Last a couple sweeps with the barycenter heuristic to reduce edge crossings.\\




\section{dag}
In the following the dag progression for different steps will be shown to visualize the changes. In order for them to be easier to follow only 5 prompts will be chosen. We can see that initially relations between the prompts are hard to glean, even with only 5 prompts comparing the text and identifying the similarities and difference is quite a chore for the human visual complex. Removing stop and function words makes it a little easier due to the text content decreasing however it's still hard and will only exponentially increase in difficulty when adding more prompts (imagine trying to compare 50 or 100 prompts "by hand").\\
Just by aggregating words at the same position and indicating their relation via edges we can already see noticeable improvements, which are only further pronounced when collapsing nodes containing "phrase words", words that appear at that position only next to a relative.\\
However we have lost the crucial information of what nodes belong together with multiple new sentences being possible when just following the graph structure, due to the an edge aggregations followed by an edge split. To remedy this a hover function is introduced where when hovering a node all the node relatives from the original data are shown. This allows for an easier overview and and comparability of the nodes, this approach can scale quite well if prompts are sufficiently similar (if all prompts would differ at every position we'd just have a prompt list again).

\begin{figure}[H]        % 'h' = place here (optional)
	\centering           % centers the image
	\includegraphics[width=\linewidth]{DAGNothingApplied2.png}
	\caption{Raw prompts with no aggregation or processing}
	\label{fig:dagPure}  % optional: for referencing
\end{figure}
\begin{figure}[H]        % 'h' = place here (optional)
	\centering           % centers the image
	\includegraphics[width=\linewidth]{DAGTokenized.png}
	\caption{Prompts with stop and some function words removed}
	\label{fig:dagTokenized}  % optional: for referencing
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth, trim=4cm 1cm 4cm 1cm, clip]{DAGNotCollapsed.png}
	\caption{DAG with nodes at same position merged with edges indicating relationships noticeably reduced size }
	\label{fig:dagMerged}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth, trim=4cm 1cm 4cm 1cm, clip]{DAGCollapsed.png}
	\caption{DAG with nodes collapsed if there are unique to that order and sequence, aswell as neighbourhing nodes collapsed if they have the same content (not the case here so no changes from second collapse)}
	\label{fig:dagCollapsed}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth, trim=4cm 1cm 4cm 1cm, clip]{DAGPathSelected.png}
	\caption{Node selected showing the path (ancestors,descendants) of the node relative to the original input data}
	\label{dagPath}
\end{figure}

\section{scatterplot}
To represent the images a scatterplot was chosen, this was due to 2D representation providing the best overview and being easily understood, while it also helps that humans associate visual closeness with belonging together or to a grouping making the distance between the points being given by similarity being very effective with similar images having a close proximity for an easy overview. Initially the scatter plot only contained dots representing the respective images with a hover functionality displaying the image and prompt. This however didn't give an initial overview over the entire graph which is why instead of just dots the images themselves as small circles were plotted providing the user with a good inital overview over the data. Furthermore zooming and panning is implemented to ease use of the scatterplot and allow the user more control of what he sees and focuses on.
\\Additionally a lens for image selection is introduced the lens can be varied in size allowing for larger or more precise selections, the selection is then highlighted and displayed in a grid next to a cursor. Ideally the images would be ordered based on similarity using for example SSIM as was the case in this work however for live interaction (the user sliding the lens over the scatterplot) SSIM or similar methods are with the current limitations of science and technology to slow, therefore the grid update sorted with a pixel based similarity measure only works in the live application for pre-computed values, which is in contrast to the goal of being able to change the prompt and generate images on the fly then adding the results to the framework immediately.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{scatterplotRawDots.png}
	\caption{ScatterPlot of 100 example iamges being plotted based on similarity }
	\label{fig:scatterDots}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{scatterplotRawHovered.png}
	\caption{ScatterPlot with node hovered (unfortunately cursor doesn't show up in screenshot) }
	\label{fig:scatterHovered}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{scatterplotImages.png}
	\caption{ScatterPlot with images as data indicators  }
	\label{fig:scatterImages}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{gridNoSSIM.png}
	\caption{ScatterPlot with Lens selection displaying grid unsorted }
	\label{fig:scatterNoSSIM}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{gridGeometric.png}
	\caption{ScatterPlot with Lens selection displaying grid sorted with SSIM}
	\label{fig:scatterNoSSIM}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{gridGeometric.png}
	\caption{ScatterPlot with Lens selection displaying grid with geometirc sorting}
	\label{fig:scatterNoSSIM}
\end{figure}








\section{graph interaction}

\chapter{implementation}
To realize the chosen design a web application utilizing d3 was implemented
\section{d3}
D3.js (Data-Driven Documents) is a JavaScript library for creating interactive and dynamic data visualizations in web environments. Unlike traditional libraries that provide fixed templates and charts, D3 gives developers fine-grained control over how data is bound to the graphical elements. It utilizes web standards such as HTML, SVG, and CSS, enabling flexible and highly customizable visualizations.

The key strength of D3 is its declarative data binding: datasets can directly drive the creation, transformation, and styling of visual elements. This makes it possible to build anything from simple bar charts to complex, interactive dashboards or exploratory visual analytics systems. Because of this versatility, D3 has become one of the mostcommonly used tools for web-based data visualization in both research and industry.

\section{general framework}
For the general framework a web-application was chosen utilizing htmt, JS and css for the frontend and python for the backend.
This allows for fast light weight prototyping and easy adpatability. 

\chapter{Conclusion and Outlook}
\label{chap:zusfas}

\section*{Outlook}

\printbibliography

All links were last followed on March 17, 2018.

\appendix
%\input{latexhints-english}

\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Versicherung
\end{document}
